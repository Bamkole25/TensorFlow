{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistilBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRLJSPnxVw0G",
        "colab_type": "text"
      },
      "source": [
        "# Creating a sequence classifier with DistilBERT\n",
        "\n",
        "\n",
        "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. To fix this, there is a new iteration of the trusty BERT model: DistilBERT. It cuts down on the density and size of BERT by a process called distillation: a technique where a large model, called the teacher, is compressed into a smaller model, called the student.\n",
        "\n",
        "NLP tasks have been massively simplified due to the advent of Huggingface's Transformer architecture. As a platform hosting 10+ Transformer architectures, ðŸ¤—/Transformers makes it very easy to use, fine-tune and compare the models that have transfigured the deep-learning for NLP field. It serves as a backend for many downstream apps that leverage transformer models and is in use in production by many different companies.\n",
        "\n",
        "Let's take a look at how to create a simple Sequence Classifier using DistilBERT and the Huggingface Transformers architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I77yh7wiXo1c",
        "colab_type": "text"
      },
      "source": [
        "**Step 1: Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dM2cMM9EDZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI9LOgxrR8G4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_pb2iOmR5LQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import transformers\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "from transformers import DistilBertTokenizer\n",
        "from transformers import glue_convert_examples_to_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eOtx1y_Xtg2",
        "colab_type": "text"
      },
      "source": [
        "**Step 2: Loading DistilBERT**\n",
        "\n",
        "Load pretrained DistilBERT models for the sequence classifier and the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X8fd8zvEaxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZeXGVyLYlXq",
        "colab_type": "text"
      },
      "source": [
        "**Step 3: See Tokenization**\n",
        "\n",
        "\n",
        "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.\n",
        "\n",
        "Here is an example of tokenization: \n",
        "\n",
        "***Lions, tigers, and bulldogs are very scary!***\n",
        "\n",
        "The tokens for this sentence would be:\n",
        "*   Lions\n",
        "*   tigers\n",
        "*   and\n",
        "*   bulldogs\n",
        "*   are\n",
        "*   very\n",
        "*   scary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Similarly, we will use the DistilBertTokenizer to tokenize our input sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADfu_vK7EcVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequence = \"Hello! Welcome to the wondrous world of Natural Language Processing.\"\n",
        "tokenized_sequence = tokenizer.tokenize(sequence)\n",
        "print(tokenized_sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6qjcUU6ZB0Z",
        "colab_type": "text"
      },
      "source": [
        "**Step 4: Loading the dataset**\n",
        "\n",
        "We will be using the Microsoft Research Paraphrase Corpus (MRPC) dataset, which is a sequence classification dataset. We get the train and validation data from the `tensorflow_datasets` package. These values are in the form of `tf.data.Dataset`, which is perfect for us.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQWVcGkQFq58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = tfds.load(\"glue/mrpc\")\n",
        "train_dataset = data[\"train\"]\n",
        "validation_dataset = data[\"validation\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcYVsnItZZ65",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at a sample from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndFcExYoF1fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(list(train_dataset.__iter__())[42])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFHoYsIjZgDo",
        "colab_type": "text"
      },
      "source": [
        "**Step 5: Preparing Dataset**\n",
        "\n",
        "First, we have to convert the dataset values into features usable by DistilBERT for training. Then, we randomly shuffle the dataset splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCIr7UHDGpIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = glue_convert_examples_to_features(train_dataset, roberta_tokenizer, 128, 'mrpc')\n",
        "train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pszCLsBQGqqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_dataset = glue_convert_examples_to_features(validation_dataset, roberta_tokenizer, 128, 'mrpc')\n",
        "validation_dataset = validation_dataset.batch(64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdUg5K7KaG6t",
        "colab_type": "text"
      },
      "source": [
        "**Step 6: Preparing the model for fine-tuning**\n",
        "\n",
        "Before fine-tuning the model, we must define a few hyperparameters that will be used during the training such as the optimizer, the loss and the evaluation metric.\n",
        "As an optimizer we'll be using Adam, which was the optimizer used during those models' pre-training. As a loss we'll be using the sparse categorical cross-entropy, and the sparse categorical accuracy as the evaluation metric.\n",
        "\n",
        "Note that the learning rate is extremely low so the model does not forget what it has already learned from the pre-training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZpMZ_IMGtil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-08)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N15k0C1laoYY",
        "colab_type": "text"
      },
      "source": [
        "**Step 7: Fine-tune away!**\n",
        "\n",
        "\n",
        "\n",
        "Using the fit() function from tf.keras, we can easily run fine-tuning on this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4gCfgDTG9uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Fine-tuning DistilBERT on MRPC\")\n",
        "history = model.fit(train_dataset, epochs=3, steps_per_epoch=100, validation_data=validation_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g-koXrla9Sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate(validation_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}