{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF 2.0 Census Dataset: Keras, tf.keras.layers + tf.data + save_keras_model",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "dsDV5jNzgbGl"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "dsDV5jNzgbGl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "metadata": {
        "id": "Y4UiAxk2gPnp",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mHF9VCProKJN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Census using tf.Keras + TF.data.Dataset in TF 2.0\n",
        "\n",
        "<table>![Keras+Tensorflow](https://avatars0.githubusercontent.com/u/15658638?s=200&v=4 =85x)\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "hZzRVxNtH-zG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial shows how to train a neural network using the Keras\n",
        "functional API locally and how to serve predictions\n",
        "from that model.\n",
        "\n",
        "Keras is a high-level API to build and train deep learning models.\n",
        "[tf.keras](https://www.tensorflow.org/guide/keras) is TensorFlow’s\n",
        "implementation of this API."
      ]
    },
    {
      "metadata": {
        "id": "iN69d4D9Flrh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "\n",
        "Using census data which contains data for a person's age, education, marital status, and occupation (the features), we will try to predict whether or not the person earns more than 50,000 USD a year (the target label). We will then train a neural network model that, given an individual's information (features), outputs a number between 0 and 1—this can be interpreted as the probability that the individual has an annual income of over 50,000 USD.\n",
        "\n",
        "**Key Point:** As a modeler and developer, think about how this data is used and the potential benefits and harm a model's predictions can cause. A model like this could reinforce societal biases and disparities. Is each feature relevant to the problem you want to solve or will it introduce bias? For more information, read about ML fairness."
      ]
    },
    {
      "metadata": {
        "id": "rgLXkyHEvTVD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "You must do several things before you can train a model:\n",
        "\n",
        "* Set up your development environment. (Skip this step if you're using\n",
        "Colaboratory.)\n",
        "\n",
        "If using Google Cloud:\n",
        "\n",
        "* Create a Google Cloud Platform (GCP) project with Billing and the necessary\n",
        "  APIs enabled.\n",
        "* Authenticate your GCP account in this notebook.\n",
        "* Create a Google Cloud Storage bucket to store your training package and your\n",
        "  trained model.\n"
      ]
    },
    {
      "metadata": {
        "id": "SZFttqMsQT9_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set up your development environment\n",
        "\n",
        "If you are using Colaboratory, skip this step.\n",
        "\n",
        "Otherwise, make sure your environment meets this notebook's requirements. You\n",
        "need the following:\n",
        "\n",
        "* Google Cloud SDK\n",
        "* Python 3\n",
        "* virtualenv\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "2. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "3. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3.\n",
        "\n",
        "4. Activate that environment. Run `pip install jupyter` in a shell to install\n",
        "   Jupyter.\n",
        "\n",
        "5. Run `jupyter notebook` in a shell to launch Jupyter.\n",
        "\n",
        "6. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "metadata": {
        "id": "4GYu05KjLYNG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set up your GCP project\n",
        "\n",
        "If you are using Colaboratory, skip this step\n",
        "\n",
        "Follow the first three steps of [these setup\n",
        "instructions](https://cloud.google.com/ml-engine/docs/tensorflow/getting-started-training-prediction#set-up-your-gcp-project)\n",
        "to setup a GCP Project, enable billing, and enable\n",
        "Compute Engine APIs. Enter the id of your project in the cell below.\n"
      ]
    },
    {
      "metadata": {
        "id": "4qxwBA4RM9Lu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"dpe-cloud-mle\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fZQUrHdXNJnk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Authenticate your GCP account\n",
        "\n",
        "**If you are not running this notebook in Colaboratory**, follow step four of\n",
        "the [setup\n",
        "instructions](https://cloud.google.com/ml-engine/docs/tensorflow/getting-started-training-prediction#set-up-your-gcp-project)\n",
        "to create a service account key and save it to your machine. Enter the path to\n",
        "your service account key as the `GOOGLE_APPLICATION_CREDENTIALS` constant in the\n",
        "cell below.\n",
        "\n",
        "**If you _are_ using Colaboratory**, run the cell below and follow the\n",
        "instructions when prompted to authenticate your account via OAuth."
      ]
    },
    {
      "metadata": {
        "id": "W9i6oektpgld",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colaboratory, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Google Cloud Storage bucket and lets us submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this notebook locally, please follow these instructions\n",
        "# to create a service account key: [TODO(alecglassford): Once we convert this to markdown, link to *this* tutorial on c.g.c, instead of the other guide]\n",
        "# https://cloud.google.com/ml-engine/docs/tensorflow/python-guide#set-up-your-gcp-project\n",
        "# Then, replace the string below with the path to your service account key\n",
        "# and run this cell to authenticate your GCP account.\n",
        "else:\n",
        "  GOOGLE_APPLICATION_CREDENTIALS='/path/to/your/service-account-key.json' #@param {type:\"string\"}\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS {GOOGLE_APPLICATION_CREDENTIALS}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EvD5KQp7Magn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the following cell to make sure the Cloud SDK uses the right project for\n",
        "all the commands in this notebook.\n",
        "\n",
        "Note: Jupyter interpolates Python variables in curly braces into shell commands."
      ]
    },
    {
      "metadata": {
        "id": "_lVYeBZJMq06",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! gcloud config set project {PROJECT_ID}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tT061irlJwkg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a Google Cloud Storage bucket\n",
        "\n",
        "In this tutorial, TensorFlow trained model save the results from your job in a Google Cloud Storage bucket.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
        "Cloud Storage buckets. You may also change the `REGION` variable. Make sure to\n",
        "[choose a region where Cloud ML Engine services are\n",
        "available](https://cloud.google.com/ml-engine/docs/tensorflow/regions), because\n",
        "you must run your Cloud ML Engine jobs in your Cloud Storage bucket's region."
      ]
    },
    {
      "metadata": {
        "id": "bTxmbDg1I0x1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"census_model\" #@param {type:\"string\"}\n",
        "BUCKET_NAME = \"tony-dev\" #@param {type:\"string\"}\n",
        "REGION = \"us-central1\" #@param [\"us-central1\", \"us-east1\", \"europe-west1\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fsmCk2dwJnLZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the following cell to create your Cloud Storage bucket. If it already exists, slip this step."
      ]
    },
    {
      "metadata": {
        "id": "160PRO3aJqLD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! gsutil mb -l {REGION} gs://{BUCKET_NAME}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GqotgKOpIIi3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install TensorFlow 2.0 preview"
      ]
    },
    {
      "metadata": {
        "id": "AJ5FYakiCYlU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! pip install -U tf-nightly-2.0-preview"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GKP7F6-EDb5_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1. Training a Keras model\n",
        "\n",
        "In this section we will build a Keras model from scratch.\n",
        "We will perform the following steps:\n",
        "- Data download\n",
        "- Data preparation\n",
        "- Data standarization\n",
        "- Model creation\n",
        "- Model training\n",
        "- Model evaluation\n",
        "- Model serving\n",
        "\n",
        "We will create the model and export it to serve requests in ML Engine."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0YNCwpCzsM5J"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup your GCP project\n",
        "\n",
        "Verify, you have follow the initial steps for setting up your project at the beginning of this notebook.\n",
        "Update your parameters accordingly."
      ]
    },
    {
      "metadata": {
        "id": "-VtUN0L5x4ql",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import libraries\n",
        "Import TensorFlow and supporting modules:"
      ]
    },
    {
      "metadata": {
        "id": "RcxfR3GfscsA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from six.moves import urllib\n",
        "\n",
        "# Software versions\n",
        "print(__import__('sys').version)\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xWZQbZQmx26U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Constants"
      ]
    },
    {
      "metadata": {
        "id": "Cx4OXeXEsh_v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Storage directory\n",
        "DATA_DIR = '/tmp/census_data/'\n",
        "\n",
        "# Download options.\n",
        "DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult'\n",
        "TRAINING_FILE = 'adult.data'\n",
        "EVAL_FILE = 'adult.test'\n",
        "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
        "EVAL_URL = '%s/%s' % (DATA_URL, EVAL_FILE)\n",
        "\n",
        "# These are the features in the dataset.\n",
        "_CSV_COLUMNS = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
        "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
        "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
        "    'income_bracket'\n",
        "]\n",
        "\n",
        "_CATEGORICAL_TYPES = {\n",
        "  'workclass': pd.api.types.CategoricalDtype(categories=[\n",
        "    'Federal-gov', 'Local-gov', 'Never-worked', 'Private', 'Self-emp-inc',\n",
        "    'Self-emp-not-inc', 'State-gov', 'Without-pay'\n",
        "  ]),\n",
        "  'marital_status': pd.api.types.CategoricalDtype(categories=[\n",
        "    'Divorced', 'Married-AF-spouse', 'Married-civ-spouse',\n",
        "    'Married-spouse-absent', 'Never-married', 'Separated', 'Widowed'\n",
        "  ]),\n",
        "  'occupation': pd.api.types.CategoricalDtype([\n",
        "    'Adm-clerical', 'Armed-Forces', 'Craft-repair', 'Exec-managerial',\n",
        "    'Farming-fishing', 'Handlers-cleaners', 'Machine-op-inspct',\n",
        "    'Other-service', 'Priv-house-serv', 'Prof-specialty', 'Protective-serv',\n",
        "    'Sales', 'Tech-support', 'Transport-moving'\n",
        "  ]),\n",
        "  'relationship': pd.api.types.CategoricalDtype(categories=[\n",
        "    'Husband', 'Not-in-family', 'Other-relative', 'Own-child', 'Unmarried',\n",
        "    'Wife'\n",
        "  ]),\n",
        "  'race': pd.api.types.CategoricalDtype(categories=[\n",
        "    'Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'\n",
        "  ]),\n",
        "  'native_country': pd.api.types.CategoricalDtype(categories=[\n",
        "    'Cambodia', 'Canada', 'China', 'Columbia', 'Cuba', 'Dominican-Republic',\n",
        "    'Ecuador', 'El-Salvador', 'England', 'France', 'Germany', 'Greece',\n",
        "    'Guatemala', 'Haiti', 'Holand-Netherlands', 'Honduras', 'Hong', 'Hungary',\n",
        "    'India', 'Iran', 'Ireland', 'Italy', 'Jamaica', 'Japan', 'Laos', 'Mexico',\n",
        "    'Nicaragua', 'Outlying-US(Guam-USVI-etc)', 'Peru', 'Philippines', 'Poland',\n",
        "    'Portugal', 'Puerto-Rico', 'Scotland', 'South', 'Taiwan', 'Thailand', \n",
        "    'Trinadad&Tobago', 'United-States', 'Vietnam', 'Yugoslavia'\n",
        "  ]),\n",
        "  'income_bracket': pd.api.types.CategoricalDtype(categories=[\n",
        "    '<=50K', '>50K'\n",
        "  ])\n",
        "}\n",
        "\n",
        "# This is the label (target) we want to predict.\n",
        "_LABEL_COLUMN = 'income_bracket'\n",
        "\n",
        "_CSV_COLUMN_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
        "                        [0], [0], [0], [''], ['']]\n",
        "\n",
        "# Use one CPU for this example\n",
        "NUM_CPUS = 1\n",
        "\n",
        "# This the training batch size\n",
        "BATCH_SIZE = 40\n",
        "# This is the number of epochs (passes over the full training data)\n",
        "EPOCHS = 40\n",
        "# Define learning rate.\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "_NUM_EXAMPLES = {\n",
        "    'train': 32561,\n",
        "    'validation': 16281,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VwAR6IewAJFJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Clean up directory each run\n",
        "! rm -rf {DATA_DIR}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bSJjhQ8ZyDae",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper function to download and clean files\n"
      ]
    },
    {
      "metadata": {
        "id": "iGorBTXWUZPy",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _download_and_clean_file(filename, url):\n",
        "  \"\"\" Downloads data from url, and makes changes to match the CSV format.\n",
        "      Removes excessive whitespace\n",
        "  \"\"\"\n",
        "  temp_file, _ = urllib.request.urlretrieve(url)\n",
        "  with tf.io.gfile.GFile(temp_file, 'r') as temp_file_object:\n",
        "    with tf.io.gfile.GFile(filename, 'w') as file_object:\n",
        "      for line in temp_file_object:\n",
        "        line = line.strip()\n",
        "        line = line.replace(', ', ',')\n",
        "        if not line or ',' not in line:\n",
        "          continue\n",
        "        if line[-1] == '.':\n",
        "          line = line[:-1]\n",
        "        line += '\\n'\n",
        "        file_object.write(line)\n",
        "  tf.io.gfile.remove(temp_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "486BEDShyK2U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Function to download Training and Evaluation files\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "POMfvQ-kWhZO",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def download(data_dir):\n",
        "  \"\"\"Download census data if it is not already present.\"\"\"\n",
        "  tf.io.gfile.makedirs(data_dir)\n",
        "\n",
        "  training_file_path = os.path.join(data_dir, TRAINING_FILE)\n",
        "  if not tf.io.gfile.exists(training_file_path):\n",
        "    _download_and_clean_file(training_file_path, TRAINING_URL)\n",
        "\n",
        "  eval_file_path = os.path.join(data_dir, EVAL_FILE)\n",
        "  if not tf.io.gfile.exists(eval_file_path):\n",
        "    _download_and_clean_file(eval_file_path, EVAL_URL)\n",
        "  print('Download is completed!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fuX5SyAOgYsG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download Census Dataset\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9Wii7NAss92J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download Census dataset: Training and test csv files.\n",
        "download(DATA_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kY0HQ06RE9zm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Verify data is downloaded successfully. You will see 2 files: adult.data and adult.test\n",
        "% ls -l {DATA_DIR}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J-phTTnUuwbL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the full path for training and test files.\n",
        "train_file = os.path.join(DATA_DIR, TRAINING_FILE)\n",
        "test_file = os.path.join(DATA_DIR, EVAL_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHSJTrDygqpS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load files into a Pandas Dataset"
      ]
    },
    {
      "metadata": {
        "id": "KHWiAYCew28Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This census data uses the value '?' for fields (column) that are missing data. \n",
        "# We use na_values to find ? and set it to NaN values.\n",
        "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
        "\n",
        "train = pd.read_csv(train_file, names=_CSV_COLUMNS, na_values=\"?\")\n",
        "test = pd.read_csv(test_file, names=_CSV_COLUMNS, na_values=\"?\")\n",
        "\n",
        "# Here's what the data looks like before we preprocess the data.\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hhsa1-6qVD0n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Drop Unused Features and Features that are Biased"
      ]
    },
    {
      "metadata": {
        "id": "A_mQNFOiZxWI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dataset information: https://archive.ics.uci.edu/ml/datasets/census+income\n",
        "\n",
        "\"\"\"\n",
        "These are columns we will not use as features for training. There are many\n",
        "reasons not to use certain attributes of data for training. Perhaps their\n",
        "values are noisy or inconsistent, or perhaps they encode bias that we do not\n",
        "want our model to learn. For a deep dive into the features of this Census\n",
        "dataset and the challenges they pose, see the Introduction to ML Fairness\n",
        "notebook: https://colab.research.google.com/github/google/eng-edu/blob/master/ml/cc/exercises/intro_to_fairness.ipynb\"\"\"\n",
        "\n",
        "UNUSED_COLUMNS = ['fnlwgt', 'education', 'gender']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6NIOrdciGKuk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Process Numerical and Categorical columns\n",
        "\n",
        "The Census datasets contains both numbers and strings\n",
        "we need to convert string data into numbers to be able to train the model."
      ]
    },
    {
      "metadata": {
        "id": "WHbJecf1Bb2r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(dataframe):  \n",
        "  \"\"\"Dataframe contains both numeric and categorical features, convert \n",
        "  categorical features to numeric.\n",
        "\n",
        "  Args:\n",
        "    dataframe: A `Pandas.Dataframe` to process.\n",
        "  \"\"\"\n",
        "\n",
        "  dataframe = dataframe.drop(columns=UNUSED_COLUMNS)\n",
        "  \n",
        "  # Convert integer valued (numeric) columns to floating point\n",
        "  numeric_columns = dataframe.select_dtypes(['int64']).columns\n",
        "  dataframe[numeric_columns] = dataframe[numeric_columns].astype('float32')\n",
        "\n",
        "  # Convert categorical columns to numeric\n",
        "  cat_columns = dataframe.select_dtypes(['object']).columns\n",
        "  # Keep categorical columns always using same values based on dict.\n",
        "  dataframe[cat_columns] = dataframe[cat_columns].apply(lambda x: x.astype(_CATEGORICAL_TYPES[x.name]))\n",
        "  dataframe[cat_columns] = dataframe[cat_columns].apply(lambda x: x.cat.codes)\n",
        "  return dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JKRLhKkPGHKC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = preprocess(train)\n",
        "test = preprocess(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YbMskdWmTCED",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Here's how the data has changed after we preprocessed it.\n",
        "# Note how columns like workclass, education, marital_status, occupation, \n",
        "# relationship, race, gender, native_country and income_bracket have been \n",
        "# changed to categorical conversion.\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OCzBX6LuCmTT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Split Features and Labels"
      ]
    },
    {
      "metadata": {
        "id": "gPq7WY51GW6M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Split train and test data with labels.\n",
        "# The pop() method will extract (copy) and remove the label column from the dataframe\n",
        "_train_x, train_y = train, train.pop(_LABEL_COLUMN)\n",
        "_test_x, test_y = test, test.pop(_LABEL_COLUMN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PnI9hf-sP9DF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reshape Label for Dataset. \n",
        "train_y = np.asarray(train_y).astype('float32').reshape((-1, 1))\n",
        "test_y = np.asarray(test_y).astype('float32').reshape((-1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1tDH7_RDAIK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Normalize data for Model Convergence"
      ]
    },
    {
      "metadata": {
        "id": "OXAMwr3dCsqd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def standardization(dataset):\n",
        "  \"\"\" Standardization of numeric fields, where all values will have mean of zero \n",
        "  and standard deviation of one. (z-score)\n",
        "\n",
        "  Args:\n",
        "    dataset: A `Pandas.Dataframe` \n",
        "  \"\"\"\n",
        "  dtypes = list(zip(dataset.dtypes.index, map(str, dataset.dtypes)))\n",
        "  # Normalize numeric columns.\n",
        "  for column, dtype in dtypes:\n",
        "      if dtype == 'float32':\n",
        "          dataset[column] -= dataset[column].mean()\n",
        "          dataset[column] /= dataset[column].std()\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "078--On5jnpW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Join all dataframes to standarize data, then split.\n",
        "all_data = pd.concat([_train_x, _test_x], keys=[0, 1])\n",
        "standardized_data = standardization(all_data)\n",
        "train_x, test_x = standardized_data.xs(0), standardized_data.xs(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6NZqE3Wlg1yg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Verify dataset features\n",
        "# Note how only the numeric fields (not categorical) have been standardized\n",
        "train_x.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1iqAXKj6YaBk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### DataFrame Length\n",
        "\n",
        "List length of training and testing data. ((32561, 32561, 16281, 16281))"
      ]
    },
    {
      "metadata": {
        "id": "BrO4NJfLGfHE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(train_x), len(train_y), len(test_x), len(test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5VdTmB8zoi8Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataset input function\n",
        "\n",
        "Keras supports tf.data.Dataset, we will be using this new functionality to process the dataset.\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
      ]
    },
    {
      "metadata": {
        "id": "QfpJpxW0KyS0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def input_fn(features, labels, shuffle, num_epochs, batch_size):\n",
        "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
        "  \n",
        "  if labels is None:\n",
        "    inputs = features\n",
        "  else:\n",
        "    inputs = (features, labels)    \n",
        "  dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
        "  \n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(num_epochs)\n",
        "\n",
        "  # We call repeat after shuffling, rather than before, to prevent separate\n",
        "  # epochs from blending together.\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vZrDBXnxggOH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a Keras Model"
      ]
    },
    {
      "metadata": {
        "id": "JY_A0yPyqU08",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll create our neural network using the Keras Sequential API. Keras is a high-level API to build and train deep learning models and is user friendly, modular and easy to extend. **tf.keras** is TensorFlow's implementation of this API and it supports such things as eager execution, **tf.data** pipelines and Estimators.\n",
        "\n",
        "Architecture wise, we'll build a logistic regressions using a deep neural network (DNN) with several hidden layers, where:\n",
        "\n",
        "- The input layer will have 100 units using the ReLU activation function.\n",
        "- The hidden layer will have 75 units using the ReLU activation function.\n",
        "- The hidden layer will have 50 units using the ReLU activation function.\n",
        "- The hidden layer will have 25 units using the ReLU activation function.\n",
        "- The output layer will have 1 units and use sigmoid function.\n",
        "- We will use the binary crossentropy loss function, and the RMSprop optimizer."
      ]
    },
    {
      "metadata": {
        "id": "j_lsOhQAvzS-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# *This may change once Keras supports Feature Columns.\n",
        "# https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/08_Taxifare_Keras_FeatureColumns_solution.ipynb\n",
        "\n",
        "def create_keras_model(input_dim, learning_rate):\n",
        "  \"\"\"Created Keras Model for Binary Classification.\"\"\"\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(100, activation=tf.nn.relu, input_shape=(input_dim,)))\n",
        "  model.add(tf.keras.layers.Dense(70, activation=tf.nn.relu))\n",
        "  model.add(tf.keras.layers.Dense(50, activation=tf.nn.relu))\n",
        "  model.add(tf.keras.layers.Dense(25, activation=tf.nn.relu))\n",
        "  # The single output node and Sigmoid activation makes this a Logistic Regression.\n",
        "  model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
        "\n",
        "  # Custom Optimizer: \n",
        "  # https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer\n",
        "  optimizer = tf.keras.optimizers.RMSprop(lr=learning_rate, \n",
        "                                          rho=0.9, \n",
        "                                          epsilon=1e-08, \n",
        "                                          decay=learning_rate/10)\n",
        "\n",
        "  # Compile Keras model\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JEOvojJsMaU4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Input dimensions\n",
        "input_dim = train_x.shape[1]\n",
        "print('Total features: {}'.format(input_dim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zdyOuwQcSjaY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the Keras Model\n",
        "\n",
        "keras_model = create_keras_model(input_dim=input_dim, learning_rate=LEARNING_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t3j1ce4VZxyc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Take a detailed look inside the model\n",
        "keras_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4cqeQttaHbZt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate\n",
        "\n",
        "After adding all the features to the model, let's train the model. Training a model is just a single command using the tf.data.Dataset API."
      ]
    },
    {
      "metadata": {
        "id": "jVoAU-13oGKb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Pass a numpy array by passing DataFrame.values\n",
        "training_dataset = input_fn(features=train_x.values, \n",
        "                    labels=train_y, \n",
        "                    shuffle=True, \n",
        "                    num_epochs=40, \n",
        "                    batch_size=BATCH_SIZE)\n",
        "\n",
        "# Pass a numpy array by passing DataFrame.values\n",
        "validation_dataset = input_fn(features=test_x.values, \n",
        "                    labels=test_y, \n",
        "                    shuffle=False, \n",
        "                    num_epochs=1, \n",
        "                    batch_size=BATCH_SIZE)                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7eLa_Yoj2rhv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Setup Learning Rate decay.\n",
        "lr_decay = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.0001 + 0.02 * math.pow(0.5, 1+epoch), verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1fExnH6bhOSC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**tf.data.Dataset** using Keras\n",
        "\n",
        "In this example we will use a tf.data.Dataset to train our Keras model.\n",
        "There are new parameters required:\n",
        "\n",
        "steps_per_epoch = Total of training samples / Training batch size\n",
        "validation_steps = Total of validation samples / Evaluation batch size\n",
        "\n",
        "This means how many batches per epoch you will yield.\n",
        "This is configured in order to guarantee:\n",
        "\n",
        "- You train your entire training set\n",
        "- You validate your entire validation set\n"
      ]
    },
    {
      "metadata": {
        "id": "MG4EvLiorMmZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = keras_model.fit(training_dataset, \n",
        "                          validation_data=validation_dataset, \n",
        "                          steps_per_epoch=int(_NUM_EXAMPLES['train']/BATCH_SIZE), \n",
        "                          validation_steps=int(_NUM_EXAMPLES['validation']/BATCH_SIZE), \n",
        "                          epochs=EPOCHS, \n",
        "                          callbacks=[lr_decay],\n",
        "                          verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dm2VTyIUhG6y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Reference:** Traditional Keras training mode.\n",
        "\n",
        "```keras_model.fit(x=train_x, y=train_y, epochs=40, validation_data=(test_x, test_y), verbose=1, callbacks=[lr_decay])```"
      ]
    },
    {
      "metadata": {
        "id": "fW7vTPm2pd2l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize Model history"
      ]
    },
    {
      "metadata": {
        "id": "lfuLSeSXucI4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Visualize History for Accuracy.\n",
        "plt.title('Keras Model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.legend(['training', 'testing'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yU1TzdlY0i-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Visualize History for Loss.\n",
        "plt.title('Keras Model loss')\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'testing'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmTiOQObmTqd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Export model for Serving"
      ]
    },
    {
      "metadata": {
        "id": "d4BaDUJzmTW_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exporting model to GCS. \n",
        "export_path = tf.keras.experimental.export(keras_model, os.path.join('gs://', BUCKET_NAME, 'keras_export'))\n",
        "export_path = export_path.decode('utf-8')\n",
        "print(\"Model exported to: \", export_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r40PI5eOHXtd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Generate online predictions"
      ]
    },
    {
      "metadata": {
        "id": "skb2HGPyHVEP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We Download sample data to verify predictions.\n",
        "! rm -rf test.*\n",
        "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/cloudml-samples/master/census/test.csv\n",
        "\n",
        "# Extract the file into a Pandas dataframe to process it for Predictions.\n",
        "predictions_df = pd.read_csv('test.csv', names=_CSV_COLUMNS)\n",
        "# Display data\n",
        "predictions_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ztlYSTphZ_Tm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preprocess data as Serving function is expecting numeric data.\n",
        "predict = preprocess(predictions_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rDR4bNrAOoyW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Split features and label. We will pass features only to Serving model.\n",
        "_predict_x, predict_y = predict, predict.pop(_LABEL_COLUMN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MNAonpCAm6-N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Concat training and test data to perform standarization.\n",
        "all_data = pd.concat([_train_x, _predict_x], keys=[0, 1])\n",
        "# Standarize predictions using training data + prediction.\n",
        "standardized_data = standardization(all_data)\n",
        "train_x, predict_x = standardized_data.xs(0), standardized_data.xs(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9K-iHSMzVubl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict_x.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IpJRFREOPeTo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Local Predictions"
      ]
    },
    {
      "metadata": {
        "id": "1dPMi4g7KbBQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Predict using Keras\n",
        "predict_x.to_csv('predictions.csv', header=False)\n",
        "! more predictions.csv\n",
        "\n",
        "predictions = keras_model.predict_classes(predict_x, verbose=1)\n",
        "print(['<=50K' if x==0 else '>=50K' for x in predictions])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z9hJPU5YHcLT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2019 The TensorFlow authors\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n"
      ]
    },
    {
      "metadata": {
        "id": "K0UXLWaBJnrY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Questions? Feedback?\n",
        "Feel free to send us an email (cloudml-feedback@google.com) if you run into any issues or have any questions/feedback!"
      ]
    }
  ]
}
