{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tuOe1ymfHZPu",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "# Autoencoder - Tensorflow 2.0 with eager mode and graph mode\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/template/notebook.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/template/notebook.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to generate images of handwritten digits using both eager execution and graph mode by training an Autoencoder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IqR2PQG4ZaZ0",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "! pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8-utnieNzHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sklearn.preprocessing as prep\n",
        "import tensorflow.keras.layers as layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1Eh-iCRVBm0p"
      },
      "source": [
        "## Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC0rqzaWNzH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def standard_scale(X_train, X_test):\n",
        "    preprocessor = prep.StandardScaler().fit(X_train)\n",
        "    X_train = preprocessor.transform(X_train)\n",
        "    X_test = preprocessor.transform(X_test)\n",
        "    return X_train, X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpBo8BR8NzH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_block_from_data(data, batch_size):\n",
        "    start_index = np.random.randint(0, len(data) - batch_size)\n",
        "    return data[start_index:(start_index + batch_size)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8OMS1JONzIG",
        "colab_type": "text"
      },
      "source": [
        "## Autoencoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-6nUMUlNzII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    '''Encodes a digit from the MNIST dataset'''\n",
        "    \n",
        "    def __init__(self,\n",
        "                n_dims,\n",
        "                name='encoder',\n",
        "                **kwargs):\n",
        "        super(Encoder,self).__init__(name=name, **kwargs)\n",
        "        self.n_dims = n_dims\n",
        "        self.n_layers = 1\n",
        "        self.encode_layer = layers.Dense(n_dims, activation='relu')\n",
        "        \n",
        "    @tf.function        \n",
        "    def call(self, inputs):\n",
        "        return self.encode_layer(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4YmUVd8NzIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    '''Decodes a digit from the MNIST dataset'''\n",
        "\n",
        "    def __init__(self,\n",
        "                n_dims,\n",
        "                name='decoder',\n",
        "                **kwargs):\n",
        "        super(Decoder,self).__init__(name=name, **kwargs)\n",
        "        self.n_dims = n_dims\n",
        "        self.n_layers = len(n_dims)\n",
        "        self.decode_middle = layers.Dense(n_dims[0], activation='relu')\n",
        "        self.recon_layer = layers.Dense(n_dims[1], activation='sigmoid')\n",
        "        \n",
        "    @tf.function        \n",
        "    def call(self, inputs):\n",
        "        x = self.decode_middle(inputs)\n",
        "        return self.recon_layer(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqCLs1UoNzIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Autoencoder(tf.keras.Model):\n",
        "    '''Vanilla Autoencoder for MNIST digits'''\n",
        "    \n",
        "    def __init__(self,\n",
        "                 n_dims=[200, 392, 784],\n",
        "                 name='autoencoder',\n",
        "                 **kwargs):\n",
        "        super(Autoencoder, self).__init__(name=name, **kwargs)\n",
        "        self.n_dims = n_dims\n",
        "        self.encoder = Encoder(n_dims[0])\n",
        "        self.decoder = Decoder([n_dims[1], n_dims[2]])\n",
        "        \n",
        "    @tf.function        \n",
        "    def call(self, inputs):\n",
        "        x = self.encoder(inputs)\n",
        "        return self.decoder(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEQGqMzyNzIe",
        "colab_type": "text"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5PSeedlNzIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHcU9qpwNzIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, _), (X_test, _) = mnist.load_data()\n",
        "X_train = tf.cast(np.reshape(X_train, (X_train.shape[0], X_train.shape[1] * X_train.shape[2])), tf.float64)\n",
        "X_test = tf.cast(np.reshape(X_test, (X_test.shape[0], X_test.shape[1] * X_test.shape[2])), tf.float64)\n",
        "\n",
        "X_train, X_test = standard_scale(X_train, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5pW1MK6NzIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices(X_train).batch(128).shuffle(buffer_size=1024)\n",
        "test_data = tf.data.Dataset.from_tensor_slices(X_test).batch(128).shuffle(buffer_size=512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8sDlNzkNzI0",
        "colab_type": "text"
      },
      "source": [
        "## Training setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duilfQyHNzI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_samples = int(len(X_train) + len(X_test))\n",
        "training_epochs = 20\n",
        "batch_size = 128\n",
        "display_step = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azYo2QWgNzI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
        "mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "loss_metric = tf.keras.metrics.Mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huIJC7iHNzI-",
        "colab_type": "text"
      },
      "source": [
        "## Eager mode training\n",
        "Using `tf.GradientTape()` to expose the tape to perform Autodifferentiation. Tensorflow \"records\" all operations executed inside the context of a \"tape\". Tensorflow then uses that tape and the gradients associated with each recorded operation to compute the gradients of a \"recorded\" computation using reverse mode differentiation. The optimizer can then be used to apply the gradients to all the trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA9huq7ZNzJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder = Autoencoder([200, 394, 784])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTqNHWmwNzJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iterate over epochs.\n",
        "for epoch in range(5):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "    for step, x_batch in enumerate(train_data):\n",
        "        with tf.GradientTape() as tape:\n",
        "          recon = autoencoder(x_batch)\n",
        "          loss = mse_loss(x_batch, recon)\n",
        "\n",
        "        grads = tape.gradient(loss, autoencoder.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_variables))\n",
        "\n",
        "        loss_metric(loss)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "          print(f'Step {step}: mean loss = {loss_metric.result()}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO4Fh-3iNzJJ",
        "colab_type": "text"
      },
      "source": [
        "## Graph mode training\n",
        "Graph mode training with `tf.keras.Model.fit` creates a computation graph and fits the training data on the keras model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1LvHDCXNzJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ae = Autoencoder([200, 392, 784])\n",
        "ae.compile(optimizer=tf.optimizers.Adam(0.01), loss='categorical_crossentropy')\n",
        "ae.fit(X_train, X_train, batch_size=64, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UhNtHfuxCGVy"
      },
      "source": [
        "## Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YrsKXcPRUvK9"
      },
      "source": [
        "- Eager Execution Basics [TF Docs Link](https://www.tensorflow.org/tutorials/eager/eager_basics)\n",
        "- Tensorflow Automatic Differentiation [TF Docs Link](https://www.tensorflow.org/tutorials/eager/automatic_differentiation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWlQ-IPpNzJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}