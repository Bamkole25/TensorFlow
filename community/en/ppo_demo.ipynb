{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppo_demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MViyl8wtGMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0-rc0\n",
        "!pip install gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hTtbFuptK7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import gym\n",
        "import numpy as np\n",
        "import scipy.signal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of2v7DeYtOlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mlp(ob_space, hidden_sizes=(32,), activation='tanh'):\n",
        "  model = tf.keras.Sequential()\n",
        "  for h in hidden_sizes[:-1]:\n",
        "    model.add(tf.keras.layers.Dense(units=h, activation=activation))\n",
        "  model.add(tf.keras.layers.Dense(units=hidden_sizes[-1]))\n",
        "  model.build(input_shape=(None,) + ob_space.shape)\n",
        "  return model\n",
        "\n",
        "class PPOAgent():\n",
        "\n",
        "  def __init__(self, ob_space, ac_space, hidden_sizes=(64, 64), activation='tanh'):\n",
        "    self.act_dim = ac_space.n\n",
        "    self.actor_mlp = mlp(ob_space=ob_space, hidden_sizes=list(hidden_sizes)+[self.act_dim])\n",
        "    self.critic_mlp = mlp(ob_space=ob_space, hidden_sizes=list(hidden_sizes)+[1])\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, observations):\n",
        "    logits = self.actor_mlp(observations)\n",
        "    logp_all = tf.nn.log_softmax(logits)\n",
        "    pi = tf.squeeze(tf.random.categorical(logits, num_samples=1, seed=0), axis=1)\n",
        "    logp_pi = tf.reduce_sum(tf.one_hot(pi, depth=self.act_dim) * logp_all, axis=1)\n",
        "    vf = self.critic_mlp(observations)\n",
        "    return pi, logp_pi, vf\n",
        "\n",
        "  @tf.function\n",
        "  def get_logp(self, observations, actions):\n",
        "    logits = self.actor_mlp(observations)\n",
        "    logp_all = tf.nn.log_softmax(logits)\n",
        "    return tf.reduce_sum(tf.one_hot(actions, depth=self.act_dim) * logp_all, axis=1)\n",
        "\n",
        "  @tf.function\n",
        "  def get_v(self, observations):\n",
        "    return tf.squeeze(self.critic_mlp(observations), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uESZyKSat7ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discount_cumsum(x, discount):\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "class PPOBuffer:\n",
        "\n",
        "    def __init__(self, ob_space, ac_space, size, gamma=0.99, lam=0.97):\n",
        "        self.obs_buf = np.zeros((size,) + ob_space.shape, dtype=ob_space.dtype)\n",
        "        self.act_buf = np.zeros((size,) + ac_space.shape, dtype=ac_space.dtype)\n",
        "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.gamma, self.lam = gamma, lam\n",
        "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
        "\n",
        "    def store(self, obs, act, rew, val, logp):\n",
        "        assert self.ptr < self.max_size\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.act_buf[self.ptr] = act\n",
        "        self.rew_buf[self.ptr] = rew\n",
        "        self.val_buf[self.ptr] = val\n",
        "        self.logp_buf[self.ptr] = logp\n",
        "        self.ptr += 1\n",
        "\n",
        "    def finish_path(self, last_val=0):\n",
        "        path_slice = slice(self.path_start_idx, self.ptr)\n",
        "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
        "        vals = np.append(self.val_buf[path_slice], last_val)\n",
        "        # the next two lines implement GAE-Lambda advantage calculation\n",
        "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
        "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
        "        # the next line computes rewards-to-go, to be targets for the value function\n",
        "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
        "        self.path_start_idx = self.ptr\n",
        "\n",
        "    def get(self):\n",
        "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
        "        self.ptr, self.path_start_idx = 0, 0\n",
        "        # the next two lines implement the advantage normalization trick\n",
        "        adv_mean = np.mean(self.adv_buf)\n",
        "        adv_std = np.std(self.adv_buf)\n",
        "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
        "        return [self.obs_buf, self.act_buf, self.adv_buf, \n",
        "                self.ret_buf, self.logp_buf]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfKTfGNk-dif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update(obs, acs, advs, rets, logp_olds, agent, opt_pi, opt_v,\n",
        "           train_pi_iters=80, train_v_iters=80, clip_ratio=0.2, target_kl=0.01):\n",
        "  actor_weights = agent.actor_mlp.trainable_weights\n",
        "  critic_weights = agent.critic_mlp.trainable_weights\n",
        "  @tf.function\n",
        "  def update_pi():\n",
        "    pi_loss = 0.\n",
        "    for i in tf.range(train_pi_iters):\n",
        "      with tf.GradientTape() as tape:\n",
        "        logp = agent.get_logp(obs, acs)\n",
        "        ratio = tf.exp(logp - logp_olds)\n",
        "        min_adv = tf.where(advs > 0, (1+clip_ratio)*advs, (1-clip_ratio)*advs)\n",
        "        pi_loss = -tf.reduce_mean(tf.minimum(ratio * advs, min_adv))\n",
        "      grads = tape.gradient(pi_loss, actor_weights)\n",
        "      opt_pi.apply_gradients(zip(grads, actor_weights))\n",
        "      kl = tf.reduce_mean(logp_olds - logp)\n",
        "      if kl > 1.5 * target_kl:\n",
        "        break\n",
        "    return pi_loss\n",
        "\n",
        "  @tf.function\n",
        "  def update_v():\n",
        "    v_loss = 0.\n",
        "    for i in tf.range(train_v_iters):\n",
        "      with tf.GradientTape() as tape:\n",
        "        v = agent.get_v(obs)\n",
        "        v_loss = tf.reduce_mean((rets - v)**2)\n",
        "      grads = tape.gradient(v_loss, critic_weights)\n",
        "      opt_v.apply_gradients(zip(grads, critic_weights))\n",
        "    return v_loss\n",
        "\n",
        "  return update_pi().numpy(), update_v().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWDxblBhvbjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ppo(env_name='CartPole-v1', steps_per_epoch=4000, epochs=50, pi_lr=3e-4, vf_lr=1e-3):\n",
        "  env = gym.make(env_name)\n",
        "  ob_space = env.observation_space\n",
        "  ac_space = env.action_space\n",
        "  agent = PPOAgent(ob_space, ac_space)\n",
        "  opt_pi, opt_v = tf.optimizers.Adam(pi_lr), tf.optimizers.Adam(vf_lr)\n",
        "  # Experience PPO buffer from SpinningUp.\n",
        "  buf = PPOBuffer(ob_space, ac_space, steps_per_epoch)\n",
        "  o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
        "\n",
        "  # Main loop: collect experience in env and update/log each epoch\n",
        "  for epoch in range(epochs):\n",
        "    Ep_Ret = []\n",
        "    for t in range(steps_per_epoch):\n",
        "      a, logp_t, v_t = [res.numpy()[0] for res in agent(o.reshape(1, -1))]\n",
        "      buf.store(o, a, r, v_t, logp_t)\n",
        "      o, r, d, _ = env.step(a)\n",
        "      ep_ret += r\n",
        "      ep_len += 1\n",
        "      if d or t==steps_per_epoch-1:\n",
        "          last_val = r if d else agent.get_v(o.reshape(1, -1)).numpy()\n",
        "          buf.finish_path(last_val)\n",
        "          Ep_Ret.append(ep_ret)\n",
        "          o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
        "\n",
        "    obs, acs, advs, rets, logp_olds = buf.get()\n",
        "    pi_loss, v_loss = update(obs, acs, advs, rets, logp_olds, agent, opt_pi, opt_v)\n",
        "    print('epoch {}, avg episode return {}'.format(epoch, np.mean(Ep_Ret)))\n",
        "\n",
        "  return agent, env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PIkgPbbyGUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent, env = ppo()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1IEQmfpzkeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obs = env.reset()\n",
        "reward = 0\n",
        "epoch = 0\n",
        "while epoch < 20:\n",
        "  action, _, _ = agent.get_pi_logpi_vf(obs.reshape(1, -1))\n",
        "  obs, r, d, _ = env.step(action.numpy()[0])\n",
        "  reward += r\n",
        "  # env.render()\n",
        "  if d:\n",
        "    print('episode reward {}'.format(reward))\n",
        "    reward = 0\n",
        "    epoch += 1\n",
        "    obs = env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftAG9n32J-LC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmv74g_Dmr_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.observation_space.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btzlHYkcms3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7jK6xh9mtk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}