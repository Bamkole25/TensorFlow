{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GPT-2.ipynb","provenance":[],"mount_file_id":"1OXsUjDfF5YoD5xS3dUNq1qJ0pOdvhRwV","authorship_tag":"ABX9TyPTQgoSu3SXwIzktQzZb8Tf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üòäThis is an example notebook for GPT-2. If you feel that the structure or content of this notebook is not clear enough, you can go to tensorflow/examples/models to download the code of the GPT-2 model and run it on your computer."],"metadata":{"id":"33wmePvcjkFB"}},{"cell_type":"markdown","source":["# Download the original warehouse code and training data"],"metadata":{"id":"vXqj9ycPltj3"}},{"cell_type":"code","source":["!git clone https://github.com/starxsky/gpt-2"],"metadata":{"id":"vw5ANxYHlmz-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GPT-2 EXAMP"],"metadata":{"id":"U3DfObMeVyNI"}},{"cell_type":"code","source":["!pip install setuptools==41.0.1\n","!pip install ftfy==5.6\n","!pip install tqdm==4.32.1\n","!pip instal Click==7.0\n","!pip install sentencepiece==0.1.83\n","!pip install tensorflow==2.7.0\n","!pip install numpy==1.16.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"jve5Abfgchkr","executionInfo":{"status":"ok","timestamp":1648119506910,"user_tz":-480,"elapsed":129540,"user":{"displayName":"Ê¥™ÁõõÈÇ¢","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06840656023760141783"}},"outputId":"03e58874-0941-4f5c-b5a7-2c6dd15f5245"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting setuptools==41.0.1\n","  Downloading setuptools-41.0.1-py2.py3-none-any.whl (575 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 575 kB 5.2 MB/s \n","\u001b[?25hInstalling collected packages: setuptools\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed setuptools-41.0.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pkg_resources"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting ftfy==5.6\n","  Downloading ftfy-5.6.tar.gz (58 kB)\n","\u001b[?25l\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 10 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 20 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 30 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 40 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58 kB 2.8 MB/s \n","\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==5.6) (0.2.5)\n","Building wheels for collected packages: ftfy\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-5.6-py3-none-any.whl size=44553 sha256=0cc2a2f7a37982917561b314d8db6d91701cdb652971d9aa24c598f70d2c096c\n","  Stored in directory: /root/.cache/pip/wheels/61/d0/cf/87de309cf05388523a6416562904c9dc556d98057c706cbc6e\n","Successfully built ftfy\n","Installing collected packages: ftfy\n","Successfully installed ftfy-5.6\n","Collecting tqdm==4.32.1\n","  Downloading tqdm-4.32.1-py2.py3-none-any.whl (49 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49 kB 2.9 MB/s \n","\u001b[?25hInstalling collected packages: tqdm\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.63.0\n","    Uninstalling tqdm-4.63.0:\n","      Successfully uninstalled tqdm-4.63.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.32.1 which is incompatible.\n","panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.32.1 which is incompatible.\n","fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.32.1 which is incompatible.\u001b[0m\n","Successfully installed tqdm-4.32.1\n","ERROR: unknown command \"instal\" - maybe you meant \"install\"\n","Collecting sentencepiece==0.1.83\n","  Downloading sentencepiece-0.1.83-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.0 MB 5.4 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.83\n","Collecting tensorflow==2.7.0\n","  Downloading tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 489.6 MB 21 kB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.2.0)\n","Collecting keras<2.8,>=2.7.0rc0\n","  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3 MB 51.9 MB/s \n","\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (13.0.0)\n","Collecting gast<0.5.0,>=0.2.1\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.17.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.44.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.15.0)\n","Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.8.0)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.21.5)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.2)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.6.3)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.14.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.10.0.2)\n","Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n","  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 463 kB 54.1 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.24.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.0.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.1.0)\n","Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.37.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0) (1.5.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.23.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.3.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (41.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.35.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.0.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.7.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.0)\n","Installing collected packages: tensorflow-estimator, keras, gast, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.8.0\n","    Uninstalling keras-2.8.0:\n","      Successfully uninstalled keras-2.8.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.0\n","    Uninstalling tensorflow-2.8.0:\n","      Successfully uninstalled tensorflow-2.8.0\n","Successfully installed gast-0.4.0 keras-2.7.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0\n","Collecting numpy==1.16.4\n","  Downloading numpy-1.16.4-cp37-cp37m-manylinux1_x86_64.whl (17.3 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17.3 MB 368 kB/s \n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.5\n","    Uninstalling numpy-1.21.5:\n","      Successfully uninstalled numpy-1.21.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray 0.18.2 requires numpy>=1.17, but you have numpy 1.16.4 which is incompatible.\n","tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.16.4 which is incompatible.\n","spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.32.1 which is incompatible.\n","scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.16.4 which is incompatible.\n","pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.16.4 which is incompatible.\n","pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.16.4 which is incompatible.\n","pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.16.4 which is incompatible.\n","pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.16.4 which is incompatible.\n","kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.16.4 which is incompatible.\n","jaxlib 0.3.2+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.16.4 which is incompatible.\n","jax 0.3.4 requires numpy>=1.19, but you have numpy 1.16.4 which is incompatible.\n","fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.32.1 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.16.4 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed numpy-1.16.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{}}]},{"cell_type":"markdown","source":["# Small plugin for writing GPT"],"metadata":{"id":"TguFmNr7cFUg"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import os \n","import math\n","import sys\n"],"metadata":{"id":"nJ89JrS1dLCg","executionInfo":{"status":"ok","timestamp":1648119510921,"user_tz":-480,"elapsed":4021,"user":{"displayName":"Ê¥™ÁõõÈÇ¢","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06840656023760141783"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### INPUT CONFIG"],"metadata":{"id":"IILcae6YfJbL"}},{"cell_type":"code","source":["\n","def shape_as_list_2(x):\n","    return [int(i) for i in tf.shape(x)]\n","\n","\n","def gelu(x):\n","    with tf.name_scope(\"gelu\"):\n","        cdf = 0.5 * (1.0 + tf.tanh(\n","            (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n","        return x * cdf\n","\n","\n","def get_padding_mask(seq):\n","    with tf.name_scope(\"Padding_Mask\"):\n","        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","\n","        # add extra dimensions to add the padding\n","        # to the attention logits.\n","        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n","\n","\n","def attention_mask(size):\n","    \"\"\"\n","    if size is 4 then it returns below matrix\n","       [[0., 1., 1., 1.],\n","        [0., 0., 1., 1.],\n","        [0., 0., 0., 1.],\n","        [0., 0., 0., 0.]]\n","\n","    \"\"\"\n","    with tf.name_scope(\"attention_mask\"):\n","        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","        return mask  # (seq_len, seq_len)\n","\n","\n","def create_masks(inp):\n","    with tf.name_scope(\"att_masking\"):\n","        att_mask = attention_mask(tf.shape(inp)[1])\n","        padding_mask = get_padding_mask(inp)\n","        mask = tf.maximum(padding_mask, att_mask)\n","\n","        return mask"],"metadata":{"id":"5O1CnnCKe78M","executionInfo":{"status":"ok","timestamp":1648119510921,"user_tz":-480,"elapsed":6,"user":{"displayName":"Ê¥™ÁõõÈÇ¢","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06840656023760141783"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Build GPT layers"],"metadata":{"id":"O5--opSkfTj8"}},{"cell_type":"code","source":["\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, att_dropout=0.1, residual_dropout=0.1, scale=True):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","        self.att_dropout = att_dropout\n","        self.residual_dropout = residual_dropout\n","        self.scale = scale\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.c_attn = Conv1d(self.d_model, self.d_model * 3)\n","        self.c_proj = Conv1d(self.d_model, self.d_model)\n","\n","    def multihead_attention(self, q, k, v, training, mask=None):\n","        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","        if self.scale:\n","            dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","            matmul_qk = matmul_qk / tf.math.sqrt(dk)\n","\n","        if mask is not None:\n","            matmul_qk += (mask * -1e9)\n","\n","        attention_weights = tf.nn.softmax(matmul_qk, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","        if training:\n","            attention_weights = tf.nn.dropout(attention_weights, rate=self.att_dropout, name=\"attn_dropout\")\n","        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","        return output, attention_weights\n","\n","    def split_heads(self, x):\n","        batch_size = tf.shape(x)[0]\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def merge_heads(self, x):\n","        batch_size = tf.shape(x)[0]\n","        x = tf.transpose(x, perm=[0, 2, 1, 3])\n","        # (batch_size, seq_len_q, num_heads, depth)\n","\n","        merged = tf.reshape(x, (batch_size, -1, self.d_model))\n","        # (batch_size, seq_len_q, d_model)\n","        return merged\n","\n","    def call(self, x, mask=None, past_layer=None, training=True):\n","        x = self.c_attn(x)\n","        query, key, value = tf.split(x, 3, axis=2)\n","\n","        query = self.split_heads(query)\n","        key = self.split_heads(key)\n","        value = self.split_heads(value)\n","\n","        if past_layer is not None:\n","            past_key, past_value = tf.unstack(past_layer, axis=1)\n","            key = tf.concat([past_key, key], axis=-2)\n","            value = tf.concat([past_value, value], axis=-2)\n","\n","        present = tf.stack([key, value], axis=1)\n","\n","        scaled_attention, attention_weights = self.multihead_attention(query, key, value, training, mask)\n","\n","        concat_attention = self.merge_heads(scaled_attention)\n","\n","        output = self.c_proj(concat_attention)  # (batch_size, seq_len_q, d_model)\n","        if training:\n","            output = tf.nn.dropout(output, rate=self.residual_dropout, name=\"resid_dropout\")\n","\n","        return output, present\n","\n","\n","\n","\n","\n","\n","\n","\n","class Conv1d(tf.keras.layers.Layer):\n","    def __init__(self, hidden_size,\n","                 filter_size,\n","                 weights_init_stdev=0.02,\n","                 weights_mean=0.0,\n","                 bias_init=0.0):\n","        super(Conv1d, self).__init__()\n","\n","        self.weights_init_stdev = weights_init_stdev\n","        self.weights_mean = weights_mean\n","        self.bias_init = bias_init\n","        self.hidden_size = hidden_size\n","        self.filter_size = filter_size\n","\n","    def build(self, input_shape):\n","        self.weight = self.add_weight(\n","            \"cov1d_weights\",\n","            shape=[self.hidden_size, self.filter_size],\n","            dtype=tf.float32,\n","            initializer=tf.random_normal_initializer(\n","                stddev=self.weights_init_stdev,\n","                mean=self.weights_mean))\n","\n","        self.bias = self.add_weight(\"conv1d_biases\",\n","                                    shape=[self.filter_size],\n","                                    initializer=tf.constant_initializer(self.bias_init))\n","        super(Conv1d, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        output_shape = [tf.shape(inputs)[0], tf.shape(inputs)[1]] + [self.filter_size]\n","        inputs = tf.reshape(inputs, [-1, self.hidden_size])  # shape [batch, seq , features] => [batch*seq, features]\n","        outputs = tf.matmul(inputs, self.weight) + self.bias\n","        outputs = tf.reshape(outputs, output_shape)  # Reshape => [batch, seq, filter_size]\n","        return outputs\n","\n","\n","class FeedForward(tf.keras.layers.Layer):\n","\n","    def __init__(self, hidden_size, filter_size, dropout_rate=0.1, activation=tf.nn.relu):\n","        super(FeedForward, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.filter_size = filter_size\n","        self.activation = activation\n","        self.dropout_rate = dropout_rate\n","\n","        self.dense_layer = Conv1d(self.hidden_size, self.filter_size)\n","        self.output_dense_layer = Conv1d(self.filter_size, self.hidden_size)\n","\n","    def call(self, x, training=False):\n","        output = self.dense_layer(x)\n","        output = self.activation(output)\n","        output = self.output_dense_layer(output)\n","\n","        if training:\n","            output = tf.nn.dropout(output, rate=self.dropout_rate, name=\"feed_forward_dropout\")\n","\n","        return output\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","class LayerNormalization(tf.keras.layers.Layer):\n","\n","    def __init__(self, hidden_size):\n","        super(LayerNormalization, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","    def build(self, input_shape):\n","        self.gamma = self.add_weight(\n","            \"layer_norm_scale\",\n","            shape=[self.hidden_size],\n","            dtype=\"float32\",\n","            initializer=tf.ones_initializer(),\n","            experimental_autocast=False)\n","        self.beta = self.add_weight(\n","            \"layer_norm_bias\",\n","            shape=[self.hidden_size],\n","            dtype=\"float32\",\n","            initializer=tf.zeros_initializer(),\n","            experimental_autocast=False)\n","        super(LayerNormalization, self).build(input_shape)\n","\n","    def call(self, x, epsilon=1e-6, input_dtype=tf.float32):\n","        mean = tf.reduce_mean(x, axis=[-1], keepdims=True)\n","        variance = tf.reduce_mean(tf.square(x - mean), axis=[-1], keepdims=True)\n","        normalized = (x - mean) * tf.math.rsqrt(variance + epsilon)\n","        return tf.cast(normalized * self.gamma + self.beta, input_dtype)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","class EmbeddingLayer(tf.keras.layers.Layer):\n","\n","    def __init__(self, vocab_size, embedding_size, initializer=None, stddev=0.01, mean=0.0):\n","        super(EmbeddingLayer, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_size = embedding_size\n","        self.stddev = stddev\n","        self.mean = mean\n","        self.initializer = initializer\n","        if self.initializer is None:\n","            self.initializer = tf.random_normal_initializer(mean=self.mean,\n","                                                            stddev=self.stddev)\n","\n","    def build(self, input_shape):\n","        with tf.name_scope(\"embedding_weights\"):\n","            self.embedding_weights = self.add_weight(\n","                \"weights\",\n","                shape=[self.vocab_size, self.embedding_size],\n","                dtype=\"float32\",\n","                initializer=self.initializer\n","            )\n","        super(EmbeddingLayer, self).build(input_shape)\n","\n","    def call(self, inputs, mode=\"embedding\", scale=False):\n","        if mode == \"embedding\":\n","            return self.embedding(inputs, scale=scale)\n","        elif mode == \"projection\":\n","            return self.projection(inputs)\n","        else:\n","            raise ValueError(\"mode {} is not valid.\".format(mode))\n","\n","    def embedding(self, inputs, scale=False):\n","        with tf.name_scope(\"embedding\"):\n","            # Create binary mask of size [batch_size, length]\n","            mask = tf.cast(tf.not_equal(inputs, 0), tf.float32)\n","            inputs = tf.cast(inputs, tf.int32)\n","            embeddings = tf.nn.embedding_lookup(self.embedding_weights, inputs)\n","            embeddings *= tf.expand_dims(mask, -1)\n","            # Scale embedding by the sqrt of the hidden size\n","            if scale:\n","                embeddings *= self.embedding_size ** 0.5\n","\n","            return embeddings\n","\n","    def projection(self, inputs):\n","        with tf.name_scope(\"output_layer\"):\n","            batch_size = tf.shape(inputs)[0]\n","            seq_len = tf.shape(inputs)[1]\n","\n","            h_flat = tf.reshape(inputs, [-1, self.embedding_size])\n","            logits = tf.matmul(h_flat, self.embedding_weights, transpose_b=True)\n","\n","            return tf.reshape(logits, [batch_size, seq_len, self.vocab_size])\n","\n","\n","class PositionEmbeddingLayer(tf.keras.layers.Layer):\n","\n","    def __init__(self, position_seq, pos_embedding_size, trainable=True, stddev=0.02, mean=0.0):\n","        super(PositionEmbeddingLayer, self).__init__()\n","        self.position_seq = position_seq\n","        self.hidden_size = pos_embedding_size\n","        self.trainable = trainable\n","        self.stddev = stddev\n","        self.mean = mean\n","\n","        if trainable:\n","            self.position_embedding = EmbeddingLayer(self.position_seq, self.hidden_size,\n","                                                     stddev=self.stddev, mean=self.mean)\n","\n","    def call(self, inputs, start=1):\n","        with tf.name_scope(\"pos_embedding\"):\n","            if self.trainable:\n","                batch_size = tf.shape(inputs)[0]\n","                batch_seq = tf.shape(inputs)[1]\n","\n","                positions = tf.reshape(tf.tile(tf.range(start, batch_seq + start), [batch_size]),\n","                                       [batch_size, batch_seq])\n","\n","                positions = tf.cast(positions, tf.int32)\n","                position_mask = tf.cast(tf.not_equal(inputs, 0), tf.int32)\n","                positions *= position_mask\n","\n","                return self.position_embedding(positions)\n","            else:\n","                return self.get_position_sinusoid(self.position_seq)\n","\n","    @staticmethod\n","    def get_position_sinusoid(seq_len, hidden_size, min_timescale=1.0, max_timescale=1.0e4):\n","        position = tf.cast(tf.range(seq_len), tf.float32)\n","        num_timescales = hidden_size // 2\n","        log_timescale_increment = (\n","                math.log(float(max_timescale) / float(min_timescale)) /\n","                (tf.cast(num_timescales, tf.float32) - 1))\n","        inv_timescales = min_timescale * tf.exp(\n","            tf.cast(tf.range(num_timescales), tf.float32) * -log_timescale_increment)\n","        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n","        signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n","        return signal\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"htohuukleD0u","executionInfo":{"status":"ok","timestamp":1648119511550,"user_tz":-480,"elapsed":633,"user":{"displayName":"Ê¥™ÁõõÈÇ¢","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06840656023760141783"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Data pipline"],"metadata":{"id":"0MUEHNljfkeW"}},{"cell_type":"code","source":["import collections\n","import tensorflow as tf\n","\n","PAD_ID = 0\n","UNKNOWN_ID = 1\n","START_ID = 3\n","END_ID = 4\n","\n","#Âä†ËΩΩËØçÊ±á\n","def load_vocab(vocab_path):\n","    vocab = collections.OrderedDict()\n","    index = 0\n","    for line in open(vocab_path, 'r').read().splitlines():\n","        vocab[line.split()[0]] = index\n","        index += 1\n","    inv_vocab = {v: k for k, v in vocab.items()}\n","    return vocab, inv_vocab\n","\n","#ËΩ¨Êç¢‰∏∫ËØçÊ±á\n","def convert_by_vocab(vocab, items):\n","    output = []\n","    for item in items:\n","        output.append(vocab[item])\n","    return output\n","\n","\n","def convert_tokens_to_ids(vocab, tokens): \n","    #‰ª§ÁâåËΩ¨Êç¢Êàêid\n","    return convert_by_vocab(vocab, tokens)\n","\n","\n","def convert_ids_to_tokens(inv_vocab, ids): \n","    #id ËΩ¨Êç¢Êàê‰ª§Áâå\n","    return convert_by_vocab(inv_vocab, ids)\n","\n","\n","\n","\n","def parse_example(serialized_example):\n","    data_fields = {\n","        \"inputs\": tf.io.VarLenFeature(tf.int64),#ËæìÂÖ•\n","        \"targets\": tf.io.VarLenFeature(tf.int64)#ÁõÆÊ†á\n","    }\n","    parsed = tf.io.parse_single_example(serialized_example, data_fields)\n","    inputs = tf.sparse.to_dense(parsed[\"inputs\"])\n","    targets = tf.sparse.to_dense(parsed[\"targets\"])\n","\n","    inputs = tf.cast(inputs, tf.int32)\n","    targets = tf.cast(targets, tf.int32)\n","\n","    return inputs, targets\n","\n","\n","def input_fn(tf_records,\n","             batch_size=32,\n","             padded_shapes=([-1], [-1]),\n","             epoch=10,\n","             buffer_size=10000):\n","\n","    if type(tf_records) is str:\n","        tf_records = [tf_records]\n","    dataset = tf.data.TFRecordDataset(tf_records, buffer_size=10000)\n","    dataset = dataset.shuffle(buffer_size=buffer_size)\n","\n","    dataset = dataset.map(parse_example,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","    dataset = dataset.padded_batch(batch_size, padded_shapes=padded_shapes)\n","    dataset = dataset.repeat(epoch)\n","    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","    return dataset\n"],"metadata":{"id":"85hrohaSfvcI","executionInfo":{"status":"ok","timestamp":1648119512098,"user_tz":-480,"elapsed":552,"user":{"displayName":"Ê¥™ÁõõÈÇ¢","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06840656023760141783"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Build GPT-2 Model"],"metadata":{"id":"0ou9T9sFf1G3"}},{"cell_type":"code","source":["import os\n","from tensorflow.python.framework import tensor_shape\n","\n","\n","_ROOT = os.path.abspath(os.path.dirname(__file__))\n","LOG_DIR = _ROOT + \"/log\"\n","\n","train_step_signature = [\n","\ttf.TensorSpec(shape=(None, None), dtype=tf.int32, name=\"Inputs\"),\n","\ttf.TensorSpec(shape=(None, None), dtype=tf.int32, name=\"Targets\")\n","]\n","\n","\n","class Gpt2(tf.keras.Model):\n","\tdef __init__(self, num_layers,\n","\t             d_model,\n","\t             num_heads,\n","\t             dff,\n","\t             max_seq_len,\n","\t             vocab_size,\n","\t             optimizer=\"adam\",\n","\t             learning_rate=1e-3,\n","\t             rev_embedding_projection=True,\n","\t             grad_clip=False,\n","\t             clip_value=1.0):\n","\t\tsuper(Gpt2, self).__init__()\n","\n","\t\tself.rev_embedding_projection = rev_embedding_projection\n","\t\tself.num_layers = num_layers\n","\t\tself.num_heads = num_heads\n","\t\tself.dff = dff\n","\t\tself.max_seq_len = max_seq_len\n","\t\tself.vocab_size = vocab_size\n","\t\tself.d_model = d_model\n","\t\tself.learning_rate = learning_rate\n","\t\tself.optimizer_t = optimizer\n","\t\tself.mirrored_strategy = None\n","\t\tself.grad_clip = grad_clip\n","\t\tself.clip_value = clip_value\n","\n","\t\tself.embedding = EmbeddingLayer(\n","\t\t\tself.vocab_size, self.d_model)\n","\n","\t\tself.pos_embedding = PositionEmbeddingLayer(\n","\t\t\tself.max_seq_len, self.d_model)\n","\n","\t\tself.decoder_layers = [DecoderLayer(self.d_model, self.num_heads, self.dff)\n","\t\t                       for _ in range(self.num_layers)]\n","\t\tself.layer_norm = LayerNormalization(self.d_model)\n","\n","\t\tif not self.rev_embedding_projection:\n","\t\t\tself.output_layer = OutputLayer(self.vocab_size)\n","\n","\t\tself.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","\t\t\tfrom_logits=True, reduction='none')\n","\n","\t\tself.accuracy_object = tf.keras.metrics.SparseCategoricalAccuracy(\n","\t\t\tname='accuracy')\n","\n","\t\tself.train_step_signature = [\n","\t\t\ttf.TensorSpec(shape=(None, None), dtype=tf.int32)]\n","\n","\tdef call(self, x, training=True, past=None):\n","\t\tx = tf.cast(x, tf.int32)\n","\t\t# self.batch_size, self.sequence = tf.shape(x)[0], tf.shape(x)[1]\n","\t\tif past is None:\n","\t\t\tpasts = [None] * self.num_layers\n","\t\telse:\n","\t\t\tpasts = past\n","\n","\t\tassert len(pasts) == self.num_layers\n","\n","\t\tatt_mask = create_masks(x)\n","\t\tpast_length = 1 if past is None else tf.shape(past)[-2]\n","\t\twith tf.name_scope(\"embeddings\"):\n","\t\t\tembedded_x = self.embedding(x)\n","\t\t\thidden_states = embedded_x + self.pos_embedding(x, start=past_length)\n","\n","\t\tpresents = []\n","\t\tfor decoder_layer, past in zip(self.decoder_layers, pasts):\n","\t\t\thidden_states, present = decoder_layer(hidden_states, training, att_mask, past=past)\n","\t\t\tpresents.append(present)\n","\n","\t\thidden_states = self.layer_norm(hidden_states)\n","\n","\t\tif self.rev_embedding_projection:\n","\t\t\tlogits = self.embedding(hidden_states, mode=\"projection\")\n","\t\telse:\n","\t\t\tlogits = self.output_layer(hidden_states)\n","\n","\t\treturn logits, presents\n","\n","\t@staticmethod\n","\tdef get_padded_accuracy(labels, logits):\n","\t\twith tf.name_scope(\"padded_accuracy\"):\n","\t\t\tweights = tf.cast(tf.not_equal(labels, 0), tf.float32)\n","\n","\t\t\toutputs = tf.cast(tf.argmax(logits, axis=-1), tf.int32)\n","\t\t\tpadded_labels = tf.cast(labels, tf.int32)\n","\n","\t\t\tnonpad_seq = tf.math.count_nonzero(weights, dtype=tf.dtypes.float32, )\n","\t\t\tacc = tf.cast(tf.equal(outputs, padded_labels), tf.float32)\n","\n","\t\t\taccuracy = tf.reduce_sum(tf.cast(acc * weights, tf.float32)) / nonpad_seq\n","\t\t\treturn tf.cast(accuracy, tf.float32)\n","\n","\tdef create_optimizer(self):\n","\t\toptimizer = self.optimizer_t.lower()\n","\t\twith tf.name_scope(\"optimizer\"):\n","\t\t\tif optimizer == \"adam\":\n","\t\t\t\tself.optimizer = tf.keras.optimizers.Adam(self.learning_rate, beta_1=0.9, beta_2=0.98,\n","\t\t\t\t                                          epsilon=1e-9)\n","\t\t\telif optimizer == \"adadelta\":\n","\t\t\t\tself.optimizer = tf.keras.optimizers.Adadelta(self.learning_rate)\n","\t\t\telif optimizer == \"rms\":\n","\t\t\t\tself.optimizer = tf.keras.optimizers.RMSprop(self.learning_rate)\n","\t\t\telse:\n","\t\t\t\tself.optimizer = tf.keras.optimizers.SGD(self.learning_rate)\n","\t\t\treturn self.optimizer\n","\n","\tdef get_loss(self, real, pred):\n","\t\twith tf.name_scope(\"loss_layer\"):\n","\t\t\tmask = tf.math.logical_not(tf.math.equal(real, 0))\n","\t\t\tloss_ = self.loss_object(real, pred)\n","\n","\t\t\twith tf.name_scope(\"loss_masking\"):\n","\t\t\t\tmask = tf.cast(mask, dtype=loss_.dtype)\n","\t\t\t\tloss_ *= mask\n","\t\t\tloss_ = tf.reduce_sum(loss_, axis=1)\n","\t\t\tsequence_avg_loss = loss_ / tf.reduce_sum(mask, axis=1)\n","\t\t\treturn sequence_avg_loss\n","\n","\t@staticmethod\n","\tdef get_perplexity(cross_entropy):\n","\t\tperplexity = tf.exp(cross_entropy)\n","\t\treturn perplexity\n","\n","\tdef create_checkpoint_manager(self, checkpoint_path, max_to_keep=5, load_model=True):\n","\t\twith tf.name_scope('checkpoint_manager'):\n","\t\t\tckpt = tf.train.Checkpoint(optimizer=self.optimizer, model=self)\n","\t\t\tself.ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=max_to_keep)\n","\n","\t\t\tif load_model:  # If want to load trained weights\n","\t\t\t\tckpt.restore(self.ckpt_manager.latest_checkpoint)\n","\t\t\t\tprint('Latest checkpoint restored...............')\n","\t\t\telse:\n","\t\t\t\tprint(\"Initializing model from scratch..........\")\n","\n","\tdef load_model(self, filepath):\n","\t\tckpt = tf.train.Checkpoint(model=self)\n","\t\tckpt_manager = tf.train.CheckpointManager(ckpt, filepath)\n","\t\tckpt.restore(ckpt_manager.latest_checkpoint)\n","\t\tprint(\"Model Restored..........................\")\n","\n","\tdef create_summary_writer(self, summary_path):\n","\t\ttrain_summary_path = summary_path + \"/train\"\n","\t\ttest_summary_path = summary_path + \"/test\"\n","\n","\t\twith tf.name_scope('summary'):\n","\t\t\tself.train_writer = tf.summary.create_file_writer(train_summary_path)\n","\t\t\tself.test_writer = tf.summary.create_file_writer(test_summary_path)\n","\n","\t\t\treturn self.train_writer, self.test_writer\n","\n","\tdef _train_step(self, inputs, targets):\n","\t\twith tf.GradientTape() as tape:\n","\t\t\tpredictions, _ = self(inputs, training=True)\n","\t\t\tloss = tf.reduce_mean(self.get_loss(targets, predictions))\n","\n","\t\twith tf.name_scope(\"gradients\"):\n","\t\t\tgradients = tape.gradient(loss, self.trainable_variables)\n","\t\t\tif self.grad_clip:\n","\t\t\t\tgradients = [(tf.clip_by_value(grad, -self.clip_value, self.clip_value))\n","\t\t\t\t             for grad in gradients]\n","\t\t\tself.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","\n","\t\tperplexity = self.get_perplexity(loss)\n","\t\tstep = self.optimizer.iterations\n","\n","\t\treturn step, loss, perplexity\n","\n","\tdef _test_step(self, inputs, targets):\n","\t\tpred, _ = self(inputs, training=False)\n","\t\tloss = self.get_loss(targets, pred)\n","\t\tperplexity = self.get_perplexity(loss)\n","\t\treturn loss, perplexity\n","\n","\t@tf.function(input_signature=train_step_signature)\n","\tdef train_step(self, inputs, targets):\n","\t\treturn self._train_step(inputs, targets)\n","\n","\t@tf.function(input_signature=train_step_signature)\n","\tdef test_step(self, inputs, targets):\n","\t\treturn self._test_step(inputs, targets)\n","\n","\tdef _distributed_train_step(self, inputs, targets):\n","\n","\t\tdef step_fn(inp, tar):\n","\t\t\twith tf.GradientTape() as tape:\n","\t\t\t\tlogits, _ = self(inp, training=True)\n","\t\t\t\tcross_entropy = self.get_loss(tar, logits)\n","\t\t\t\tloss = tf.reduce_sum(cross_entropy) * (1.0 / self.global_batch_size)  # Divided By Global Batch Size\n","\n","\t\t\twith tf.name_scope(\"gradients\"):\n","\t\t\t\tgradients = tape.gradient(loss, self.trainable_variables)\n","\t\t\t\tif self.grad_clip:\n","\t\t\t\t\tgradients = [(tf.clip_by_value(grad, -self.clip_value, self.clip_value))\n","\t\t\t\t\t             for grad in gradients]\n","\t\t\t\tself.optimizer.apply_gradients(list(zip(gradients, self.trainable_variables)))\n","\t\t\treturn cross_entropy\n","\n","\t\tper_example_losses = self.mirrored_strategy.run(\n","\t\t\tstep_fn, args=(inputs, targets))\n","\n","\t\tmean_loss = self.mirrored_strategy.reduce(\n","\t\t\ttf.distribute.ReduceOp.MEAN, per_example_losses, axis=0)\n","\t\t# If you get error in distributed mode try using SUM instead of MEAN.\n","\n","\t\tperplexity = self.get_perplexity(mean_loss)\n","\t\tstep = self.optimizer.iterations\n","\n","\t\treturn step, mean_loss, perplexity\n","\n","\tdef _distributed_test_step(self, inputs, targets):\n","\t\tdef step_fn(inp, tar):\n","\t\t\tlogits, _ = self(inp, training=False)\n","\t\t\tcross_entropy = self.get_loss(tar, logits)\n","\t\t\treturn cross_entropy\n","\n","\t\tper_example_losses = self.mirrored_strategy.run(\n","\t\t\tstep_fn, args=(inputs, targets))\n","\n","\t\tmean_loss = self.mirrored_strategy.reduce(\n","\t\t\ttf.distribute.ReduceOp.MEAN, per_example_losses, axis=0)\n","\t\t# If you get error in distributed mode try using SUM instead of MEAN.\n","\t\tperplexity = self.get_perplexity(mean_loss)\n","\n","\t\treturn mean_loss, perplexity\n","\n","\t@tf.function(experimental_relax_shapes=True)\n","\tdef distributed_train_step(self, inputs, targets):\n","\t\treturn self._distributed_train_step(inputs, targets)\n","\n","\t@tf.function(experimental_relax_shapes=True)\n","\tdef distributed_test_step(self, inputs, targets):\n","\t\treturn self._distributed_test_step(inputs, targets)\n","\n","\tdef get_train_test_function(self, graph_mode=False):\n","\t\tif graph_mode:\n","\t\t\tprint(\"Running in graph mode.............\")\n","\t\t\ttrain_fuc = self.train_step\n","\t\t\ttest_fuc = self.test_step\n","\t\telse:\n","\t\t\tprint(\"Running in eager mode.............\")\n","\t\t\ttrain_fuc = self._train_step\n","\t\t\ttest_fuc = self._test_step\n","\t\treturn train_fuc, test_fuc\n","\n","\tdef get_distributed_train_test_function(self, graph_mode=False):\n","\t\tif graph_mode:\n","\t\t\tprint(\"Running in graph mode.............\")\n","\t\t\ttrain_fuc = self.distributed_train_step\n","\t\t\ttest_fuc = self.distributed_test_step\n","\t\telse:\n","\t\t\tprint(\"Running in eager mode.............\")\n","\t\t\ttrain_fuc = self._distributed_train_step\n","\t\t\ttest_fuc = self._distributed_test_step\n","\t\treturn train_fuc, test_fuc\n","\n","\tdef fit(self, train_dataset, graph_mode):\n","\t\tif self.mirrored_strategy is None:\n","\t\t\ttrain_dataset, test_dataset = train_dataset\n","\t\t\ttrain_func, test_func = self.get_train_test_function(graph_mode)\n","\t\t\ttf.summary.trace_on(graph=True, profiler=False)\n","\t\t\tfor (_, (inputs, targets)) in enumerate(train_dataset):\n","\t\t\t\tstep, loss, perplexity = train_func(inputs, targets)\n","\t\t\t\tif step % 100 == 0:\n","\t\t\t\t\tself.log_summary(self.train_writer,\n","\t\t\t\t\t                 step.numpy(),\n","\t\t\t\t\t                 loss.numpy(),\n","\t\t\t\t\t                 perplexity.numpy())\n","\n","\t\t\t\tif step == 0:\n","\t\t\t\t\twith self.train_writer.as_default():\n","\t\t\t\t\t\ttf.summary.trace_export(\n","\t\t\t\t\t\t\tname=\"gpt-2\",\n","\t\t\t\t\t\t\tstep=0,\n","\t\t\t\t\t\t\t)\n","\n","\t\t\t\tif step % 500 == 0:\n","\t\t\t\t\tlosses = []\n","\t\t\t\t\tperplexities = []\n","\t\t\t\t\tfor (test_step, (test_inputs, test_targets)) in enumerate(test_dataset):\n","\t\t\t\t\t\ttest_loss, test_perplexity = test_func(test_inputs, test_targets)\n","\t\t\t\t\t\tlosses.append(test_loss)\n","\t\t\t\t\t\tperplexities.append(test_perplexity)\n","\n","\t\t\t\t\t\tif test_step == 100:\n","\t\t\t\t\t\t\tbreak\n","\n","\t\t\t\t\ttest_loss = np.mean(np.array(losses))\n","\t\t\t\t\ttest_perplexity = np.mean(np.array(perplexities))\n","\n","\t\t\t\t\tself.log_summary(self.test_writer,\n","\t\t\t\t\t                 step.numpy(),\n","\t\t\t\t\t                 test_loss,\n","\t\t\t\t\t                 test_perplexity,\n","\t\t\t\t\t                 result_type=\"Test\")\n","\n","\t\t\t\t\tckpt_save_path = self.ckpt_manager.save()\n","\t\t\t\t\tprint('Saving checkpoint for step {} at {}'.format(step.numpy(),\n","\t\t\t\t\t                                                   ckpt_save_path))\n","\t\telse:\n","\t\t\twith self.mirrored_strategy.scope():\n","\t\t\t\ttrain_dataset, test_dataset = train_dataset\n","\t\t\t\ttrain_func, test_func = self.get_distributed_train_test_function(graph_mode)\n","\t\t\t\ttf.summary.trace_on(graph=True, profiler=False)\n","\t\t\t\tfor (step, (inputs, targets)) in enumerate(train_dataset):\n","\t\t\t\t\tstep, loss, perplexity = train_func(inputs, targets)\n","\n","\t\t\t\t\tif step % 100 == 0:\n","\t\t\t\t\t\tself.log_summary(self.train_writer,\n","\t\t\t\t\t\t                 step,\n","\t\t\t\t\t\t                 loss,\n","\t\t\t\t\t\t                 perplexity)\n","\n","\t\t\t\t\tif step == 0:\n","\t\t\t\t\t\twith self.train_writer.as_default():\n","\t\t\t\t\t\t\ttf.summary.trace_export(\n","\t\t\t\t\t\t\t\tname=\"gpt-2\",\n","\t\t\t\t\t\t\t\tstep=0,\n","\t\t\t\t\t\t\t\t)\n","\n","\t\t\t\t\tif step % 500 == 0:\n","\t\t\t\t\t\tlosses = []\n","\t\t\t\t\t\tperplexities = []\n","\t\t\t\t\t\tfor (test_step, (test_inputs, test_targets)) in enumerate(test_dataset):\n","\t\t\t\t\t\t\ttest_loss, test_perplexity = test_func(test_inputs, test_targets)\n","\t\t\t\t\t\t\tlosses.append(test_loss)\n","\t\t\t\t\t\t\tperplexities.append(test_perplexity)\n","\n","\t\t\t\t\t\t\tif test_step == 100:\n","\t\t\t\t\t\t\t\tbreak\n","\n","\t\t\t\t\t\ttest_loss = np.mean(np.array(losses))\n","\t\t\t\t\t\ttest_perplexity = np.mean(np.array(perplexities))\n","\n","\t\t\t\t\t\tself.log_summary(self.test_writer,\n","\t\t\t\t\t\t                 step,\n","\t\t\t\t\t\t                 test_loss,\n","\t\t\t\t\t\t                 test_perplexity,\n","\t\t\t\t\t\t                 result_type=\"Test\")\n","\n","\t\t\t\t\t\tckpt_save_path = self.ckpt_manager.save()\n","\t\t\t\t\t\tprint('Saving checkpoint for step {} at {}'.format(step.numpy(),\n","\t\t\t\t\t\t                                                   ckpt_save_path))\n","\n","\t@staticmethod\n","\tdef log_summary(tf_writer, step, loss, perplexity, result_type=\"Train\"):\n","\t\tprint(result_type + ':- Step {}, Loss {:.4f}, Perplexity {:.4f}'.format(\n","\t\t\tstep, loss, perplexity))\n","\t\twith tf_writer.as_default():\n","\t\t\ttf.summary.scalar(\"loss\", loss, step=step)\n","\t\t\ttf.summary.scalar(\"perplexity\", perplexity, step=step)\n","\n","\n","class OutputLayer(tf.keras.layers.Layer):\n","\tdef __init__(self, output_dim, proj_weights=None, kernel_initializer=None):\n","\t\tsuper(OutputLayer, self).__init__()\n","\t\tself.proj_weights = proj_weights\n","\t\tself.output_dim = output_dim\n","\t\tself.layer_weights = None\n","\t\tself.kernel_initializer = kernel_initializer\n","\n","\tdef build(self, input_shape):\n","\t\tif self.proj_weights is None:\n","\t\t\tinput_dim = tensor_shape.dimension_value(input_shape[-1])\n","\t\t\tself.layer_weights = self.add_weight(\n","\t\t\t\t'output_layer_weights',\n","\t\t\t\tshape=[input_dim, self.output_dim],\n","\t\t\t\tinitializer=self.kernel_initializer,\n","\t\t\t\ttrainable=True)\n","\t\tsuper(OutputLayer, self).build(input_shape)\n","\n","\tdef call(self, x):\n","\t\tbatch, sequence, d_model = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[-1]\n","\t\th_flat = tf.reshape(x, [-1, d_model])\n","\n","\t\tif self.proj_weights is None:\n","\t\t\tout = tf.matmul(h_flat, self.layer_weights)\n","\t\telse:\n","\t\t\tout = tf.matmul(h_flat, self.porj_weights, transpose_b=True)\n","\t\tout = tf.reshape(out, [batch, sequence, self.output_dim])\n","\t\treturn out\n","\n","\n","class DecoderLayer(tf.keras.layers.Layer):\n","\tdef __init__(self, d_model, num_heads, dff,\n","\t             dr_rate=0.1):\n","\t\tsuper(DecoderLayer, self).__init__()\n","\t\tself.d_model = d_model\n","\t\tself.num_heads = num_heads\n","\t\tself.dff = dff\n","\t\tself.dr_rate = dr_rate\n","\n","\t\tself.mha = MultiHeadAttention(self.d_model, self.num_heads)\n","\t\tself.feed_forward = FeedForward(self.d_model, self.dff, self.dr_rate)\n","\t\tself.layer_norm1 = LayerNormalization(self.d_model)\n","\t\tself.layer_norm2 = LayerNormalization(self.d_model)\n","\n","\tdef call(self, x, training, mask, past=None):\n","\t\tout, present = self.mha(self.layer_norm1(x), mask=mask, past_layer=past,\n","\t\t                        training=training)  # (batch_size, input_seq_len, d_model)\n","\t\twith tf.name_scope(\"residual_conn\"):\n","\t\t\tx = x + out\n","\t\tout = self.feed_forward(self.layer_norm2(x), training=training)  # (batch_size, input_seq_len, d_model)\n","\t\twith tf.name_scope(\"residual_conn\"):\n","\t\t\tx = x + out\n","\t\treturn x, present\n"],"metadata":{"id":"uOeo5zcdf6iI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Build Sampler"],"metadata":{"id":"MYPsnhxtikXC"}},{"cell_type":"code","source":["def argmax(logits):\n","\treturn tf.argmax(logits)\n","\n","\n","def top_k_logits(logits, k):\n","\tif k == 0:\n","\t\treturn logits\n","\n","\tvalues, _ = tf.nn.top_k(logits, k=k)\n","\tmin_values = values[:, -1]\n","\n","\treturn tf.where(\n","\t\tlogits < min_values,\n","\t\ttf.ones_like(logits, dtype=logits.dtype) * -1e10,\n","\t\tlogits\n","\t)\n","\n","\n","def top_p_logits(logits, p):\n","\t\"\"\"Took from OpenAI GPT-2 Implememtation\"\"\"\n","\tbatch = tf.shape(logits)[0]\n","\tsorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)\n","\tcumulative_probs = tf.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n","\tindices = tf.stack([\n","\t\ttf.range(0, batch),\n","\t\ttf.maximum(tf.reduce_sum(tf.cast(cumulative_probs <= p, tf.int32), axis=-1) - 1, 0),\n","\t], axis=-1)\n","\tmin_values = tf.gather_nd(sorted_logits, indices)\n","\treturn tf.where(\n","\t\tlogits < min_values,\n","\t\ttf.ones_like(logits) * -1e10,\n","\t\tlogits,\n","\t)\n","\n","\n","class SequenceGenerator:\n","\n","\tdef __init__(self, model_path, model_param, vocab_path):\n","\t\tself.sp = None\n","\t\tself.model = None\n","\t\tself.model_path = model_path\n","\t\tself.model_param = model_param\n","\t\tself.vocab_path = vocab_path\n","\n","\tdef load_weights(self):\n","\t\twith open(self.model_param) as f:\n","\t\t\tparam = json.load(f)\n","\t\tself.model = Gpt2(param['num_layers'],\n","\t\t\t\t\t\t  param['d_model'],\n","\t\t\t\t\t\t  param['num_heads'],\n","\t\t\t\t\t\t  param['dff'],\n","\t\t\t\t\t\t  param['max_seq_len'],\n","\t\t\t\t\t\t  param['vocab_size'])\n","\n","\t\tckpt = tf.train.Checkpoint(model=self.model)\n","\n","\t\tckpt_manager = tf.train.CheckpointManager(ckpt, self.model_path, max_to_keep=1)\n","\n","\t\tckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n","\t\tprint('Model weights loaded into memory')\n","\n","\t\tself.sp = spm.SentencePieceProcessor()\n","\t\tself.sp.load(self.vocab_path)\n","\n","\tdef sample_sequence(self,\n","\t\t\t\t\t\tcontext=None,\n","\t\t\t\t\t\tseq_len=512,\n","\t\t\t\t\t\tbos=3,\n","\t\t\t\t\t\teos=4,\n","\t\t\t\t\t\ttemperature=1,\n","\t\t\t\t\t\ttop_k=8,\n","\t\t\t\t\t\ttop_p=8,\n","\t\t\t\t\t\tnucleus_sampling=True):\n","\n","\t\tif context == None:\n","\t\t\tprint(\"Give some context to model.................\")\n","\t\t\treturn\n","\t\tcontext = tf.expand_dims(([bos] + self.sp.encode_as_ids(context)), 0)\n","\t\tprev = context\n","\t\toutput = context\n","\t\tpast = None\n","\t\tfor i in range(seq_len):\n","\t\t\tlogits, past = self.model(prev, training=False, past=past)\n","\t\t\t# print(logits)\n","\t\t\tlogits = logits[:, -1, :] / tf.cast(temperature, tf.float32)\n","\t\t\t# print(logits)\n","\t\t\tlogits = top_k_logits(logits, k=top_k)\n","\t\t\t# print(logits)\n","\t\t\tif nucleus_sampling:\n","\t\t\t\tlogits = top_p_logits(logits, p=top_p)\n","\n","\t\t\tsamples = tf.random.categorical(logits, num_samples=1, dtype=tf.int32)\n","\t\t\t# print(samples)\n","\t\t\tif tf.equal(samples, eos):\n","\t\t\t\t# print(\"Predicted end of sequence.\")\n","\t\t\t\tbreak\n","\n","\t\t\t# print(\"shape.........\")\n","\t\t\t# print(tf.shape(output))\n","\t\t\t# print(tf.shape(samples))\n","\t\t\toutput = tf.concat([output, samples], axis=-1)\n","\t\t\tprev = samples\n","\t\t\t# print(tf.shape(output))\n","\t\t\t# print(output)\n","\n","\t\t# print(\"--------------------------\")\n","\t\tresult = tf.squeeze(output, axis=0)\n","\t\tpred = [int(i) for i in result]\n","\t\tgenerated_seq = self.sp.decode_ids(pred[1:])\n","\t\tgenerated_seq = generated_seq.replace(\"[SEP]\", \"\").strip()\n","\t\tgenerated_seq = ' '.join(generated_seq.split())\n","\t\treturn generated_seq"],"metadata":{"id":"2OR6WoRTij1q","executionInfo":{"status":"aborted","timestamp":1648119514069,"user_tz":-480,"elapsed":5,"user":{"displayName":"Ê¥™ÁõõÈÇ¢","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06840656023760141783"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# pre_process"],"metadata":{"id":"wS1DdglvgV5c"}},{"cell_type":"markdown","source":["Please pay attention! ! The path here needs to be changed to your path. Before this, please put the data folder in the specified location, and then put the path of the file here.\n","\n","```\n","_ROOT = os.path.abspath(os.path.dirname(__file__))\n","PROCESS_DATA_PATH = _ROOT + \"/data/processed.txt\"\n","BPE_TSV_PATH = _ROOT + \"/data/bpe_spm.tsv\"\n","BPE_MODEL_PATH = _ROOT + \"/data/bpe_model\"\n","TF_RECORDS = _ROOT + \"/data/tf_records/\"\n","BOS_ID = 3\n","EOS_ID = 4\n","\n","```\n","\n"],"metadata":{"id":"kd3ToLmQhV4a"}},{"cell_type":"code","source":["!pip install ftfy sentencepiece"],"metadata":{"id":"eGttvGj7iL3A","executionInfo":{"status":"aborted","timestamp":1648119514070,"user_tz":-480,"elapsed":6,"user":{"displayName":"Ê¥™ÁõõÈÇ¢","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06840656023760141783"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","import datetime\n","import glob\n","import os\n","from collections import Counter\n","\n","import click\n","import numpy as np\n","import sentencepiece as spm\n","import tensorflow as tf\n","import tqdm\n","from ftfy import fix_text\n","\n","_ROOT = os.path.abspath(os.path.dirname(__file__))\n","PROCESS_DATA_PATH = _ROOT + \"/data/processed.txt\"\n","BPE_TSV_PATH = _ROOT + \"/data/bpe_spm.tsv\"\n","BPE_MODEL_PATH = _ROOT + \"/data/bpe_model\"\n","TF_RECORDS = _ROOT + \"/data/tf_records/\"\n","BOS_ID = 3\n","EOS_ID = 4\n","\n","\n","def process_text(text_files):\n","\tprint(\"Pre-processing the text data.....\")\n","\tfile_writer = open(PROCESS_DATA_PATH, \"w\",encoding='utf-8')\n","\tfor file_name in tqdm.tqdm(text_files):\n","\t\tfr = open(file_name, 'r',encoding='utf-8')\n","\t\tfile_writer.writelines([fix_text(line, normalization='NFKC') for line in fr.readlines()])\n","\t\tfr.close\n","\tfile_writer.close()\n","\n","\n","def train_byte_pair_encoding(vocab_size):\n","\tprint(\"Training BytePair encoding......\")\n","\ttoken_dict = Counter()\n","\twith open(PROCESS_DATA_PATH, 'r',encoding='utf-8') as fr:\n","\t\tfor line in tqdm.tqdm(fr):\n","\t\t\ttoken_dict.update(line.lower().split())\n","\n","\twith open(BPE_TSV_PATH, 'w', newline='',encoding='utf-8') as f_output:\n","\t\ttsv_output = csv.writer(f_output, delimiter='\\t')\n","\t\tfor word in token_dict:\n","\t\t\ttsv_output.writerow([word, token_dict[word]])\n","\n","\tspmcmd = '--input={spm_input} --model_prefix={spm_model} --input_format=tsv --vocab_size={vocab_size} --user_defined_symbols=[SEP],[BOS],[EOS] --hard_vocab_limit=false --model_type=bpe --pad_id=0 --unk_id=1 --bos_id=-1 --eos_id=-1 --pad_piece=[PAD] --unk_piece=[UNK]'.format(\n","\t\tspm_input=BPE_TSV_PATH, spm_model=BPE_MODEL_PATH, vocab_size=vocab_size)\n","\tspm.SentencePieceTrainer.train(spmcmd)\n","\n","\n","def _int64_feature(value):\n","\treturn tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n","\n","\n","def serialize_example(inputs, targets):\n","\tfeature = {\n","\t\t'inputs': _int64_feature(inputs),\n","\t\t'targets': _int64_feature(targets)\n","\t}\n","\texample_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n","\treturn example_proto.SerializeToString()\n","\n","\n","def create_tf_records(min_seq_len, max_seq_len, per_file_limit=5000):\n","\tprint(\"Creating TF Records...............\")\n","\ts = spm.SentencePieceProcessor()\n","\ts.Load(BPE_MODEL_PATH + \".model\")\n","\tif not os.path.exists(TF_RECORDS):\n","\t\tos.makedirs(TF_RECORDS)\n","\tfilename = TF_RECORDS + str(datetime.datetime.now().timestamp()) + \".tfrecord\"\n","\ttf_writer = tf.io.TFRecordWriter(filename)\n","\tdoc_counts = 0\n","\twith open(PROCESS_DATA_PATH, 'r',encoding='utf-8') as f:\n","\t\tfor line in tqdm.tqdm(f):\n","\t\t\tencoded_id = s.encode_as_ids(line)\n","\t\t\tif max_seq_len > len(encoded_id) > min_seq_len:\n","\t\t\t\tinputs = np.array([BOS_ID] + encoded_id)\n","\t\t\t\ttargets = np.array(encoded_id + [EOS_ID])\n","\n","\t\t\t\texample = serialize_example(inputs, targets)\n","\t\t\t\ttf_writer.write(example)\n","\t\t\t\tdoc_counts += 1\n","\t\t\tif doc_counts >= per_file_limit:\n","\t\t\t\ttf_writer.write(example)\n","\t\t\t\tdoc_counts = 0\n","\t\t\t\ttf_writer.close()\n","\t\t\t\tfilename = TF_RECORDS + str(datetime.datetime.now().timestamp()) + \".tfrecord\"\n","\t\t\t\ttf_writer = tf.io.TFRecordWriter(filename)\n","\n","\n","@click.command()\n","@click.option('--data-dir', type=str, default=\"./data/scraped\", show_default=True, help=\"training data path\")\n","@click.option('--vocab-size', type=int, default=24512, show_default=True, help=\"byte pair vocab size\")\n","@click.option('--min-seq-len', type=int, default=15, show_default=True, help=\"minimum sequence length\")\n","@click.option('--max-seq-len', type=int, default=512, show_default=True, help=\"minimum sequence length\")\n","def train(data_dir, vocab_size, min_seq_len, max_seq_len):\n","\t# text_files = glob.glob((_ROOT + data_dir + \"/*.txt\"))\n","\ttext_files = glob.glob((data_dir + \"/*.txt\"))\n","\t# print(text_files)\n","\tprocess_text(text_files)\n","\ttrain_byte_pair_encoding(vocab_size)\n","\tcreate_tf_records(min_seq_len, max_seq_len)\n","\tprint(\"Pre-processing is done............\")\n","\n","\n","if __name__ == \"__main__\":\n","\ttrain()\n"],"metadata":{"id":"I3nfHa-2gVUT","executionInfo":{"status":"aborted","timestamp":1648119514071,"user_tz":-480,"elapsed":7,"user":{"displayName":"Ê¥™ÁõõÈÇ¢","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06840656023760141783"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train GPT-2 Model"],"metadata":{"id":"PtODuEWTjOP2"}},{"cell_type":"code","source":["import glob\n","import json\n","\n","\n","\n","from data_pipeline import input_fn\n","from gpt2_model import *\n","\n","_ROOT = os.path.abspath(os.path.dirname(__file__))\n","LOG_DIR = _ROOT + \"/log\"\n","MODEL_DIR = _ROOT + \"/model\"\n","\n","\n","\n","#num_layers = 8\n","#embedding_size = 768\n","#num_heads = 8\n","#dff =3072\n","#max_seq_len = 515 \n","#vocab_size = 24512\n","#optimizer = \"adam\"\n","#batch_size = 8\n","#learning_rate = 0.001\n","#graph_mode = False\n","#distributed = False\n","\n","\n","def train(num_layers = 8, embedding_size = 768, num_heads = 8, dff = 3072, max_seq_len = 515, vocab_size = 24512,\n","          optimizer = \"adam\", batch_size = 8, learning_rate = 0.001, graph_mode = False, distributed = False):\n","\tpar_map = {\"num_layers\": num_layers, \"d_model\": embedding_size,\n","\t           \"num_heads\": num_heads, \"dff\": dff,\n","\t           \"max_seq_len\": max_seq_len, \"vocab_size\": vocab_size}\n","\n","\t# exp_name = \"_\".join(['{}_{}'.format(k, v) for k, v in par_map.items()])\n","\n","\tif not os.path.exists(MODEL_DIR):\n","\t\tos.makedirs(MODEL_DIR)\n","\n","\twith open(MODEL_DIR + '/model_par.json', 'w') as f:\n","\t\tjson.dump(par_map, f)\n","\n","\ttf_records = glob.glob((_ROOT + \"/data/tf_records/*.tfrecord\"))\n","\ttrain_percent = int(len(tf_records) * (85 / 100))\n","\n","\tprint(\"No. of tf records:- \", len(tf_records))\n","\ttrain_tf_records = tf_records[:train_percent]\n","\ttest_tf_records = tf_records[train_percent:]\n","\n","\ttrain_dataset = input_fn(train_tf_records, batch_size=batch_size)\n","\ttest_dataset = input_fn(test_tf_records, batch_size=batch_size)\n","\n","\tif distributed:\n","\t\tmirrored_strategy = tf.distribute.MirroredStrategy()\n","\t\ttrain_dataset = mirrored_strategy.experimental_distribute_dataset(train_dataset)\n","\t\ttest_dataset = mirrored_strategy.experimental_distribute_dataset(test_dataset)\n","\n","\t\t\n","\t\t\n","\t\twith mirrored_strategy.scope():\n","\t\t\t\n","\t\t\tmodel = Gpt2(num_layers, embedding_size, num_heads, dff, max_seq_len, vocab_size,\n","\t\t\t             optimizer=optimizer, learning_rate=learning_rate)\n","\t\t\tmodel.create_optimizer()\n","\t\t\tmodel.create_checkpoint_manager(MODEL_DIR)\n","\t\t\tmodel.create_summary_writer(LOG_DIR)\n","\n","\t\t\t\n","\t\tmodel.mirrored_strategy = mirrored_strategy\n","\t\tmodel.global_batch_size = tf.cast(batch_size, tf.float32)\n","\t\t\n","\t\t\n","\t\t\n","\t\t\n","\t\t\n","\t\t\n","\telse:\n","\t\tmodel = Gpt2(num_layers, embedding_size, num_heads, dff, max_seq_len, vocab_size,\n","\t\t             optimizer=optimizer, learning_rate=learning_rate)\n","\t\tmodel.create_optimizer()\n","\t\tmodel.create_checkpoint_manager(MODEL_DIR)\n","\t\tmodel.create_summary_writer(LOG_DIR)\n","\n","\tmodel.fit([train_dataset, test_dataset], graph_mode)\n","\tprint(\"===============>>>>>>>>>>>>>>>Done!11\")\n","\n","\n","if __name__ == \"__main__\":\n","\ttrain()"],"metadata":{"id":"PEWim5xJjR5v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# sequence gen"],"metadata":{"id":"4z1tsXLsix0k"}},{"cell_type":"code","source":["# this file was gener the sequences\n","from sample import SequenceGenerator\n","import click\n","\n","\n","@click.command()\n","@click.option('--model-path', type=str, default=\"./model\", show_default=True, help=\"Model Path\")\n","@click.option('--model-param', type=str, default=\"./model/model_par.json\", show_default=True, help=\"Model Parm\")\n","@click.option('--vocab', type=str, default=\"./data/bpe_model.model\", show_default=True, help=\"Vocab\")\n","@click.option('--seq-len', type=int, default=512, show_default=True, help=\"seq_len\")\n","@click.option('--temperature', type=float, default=1.0, show_default=True, help=\"seq_len\")\n","@click.option('--top-k', type=int, default=8, show_default=True, help=\"seq_len\")\n","@click.option('--top-p', type=float, default=0.9, show_default=True, help=\"seq_len\")\n","@click.option('--nucleus_sampling', type=bool, default=False, show_default=True, help=\"seq_len\")\n","@click.option('--context', type=str, default=\"sample context\", show_default=True, help=\"Context given to model\")\n","\n","\n","def sequence_gen(model_path, model_param, vocab, seq_len, temperature, top_k, top_p, nucleus_sampling, context):\n","\tsg = SequenceGenerator(model_path, model_param, vocab)\n","\tsg.load_weights()\n","\tgenerated_seq = sg.sample_sequence(context,\n","\t\t\t\t\t\t\t\t\t   seq_len=seq_len,\n","\t\t\t\t\t\t\t\t\t   temperature=temperature,\n","\t\t\t\t\t\t\t\t\t   top_k=top_k,\n","\t\t\t\t\t\t\t\t\t   top_p=top_p,\n","\t\t\t\t\t\t\t\t\t   nucleus_sampling=nucleus_sampling)\n","\tprint(\"<<<<<<<<<===================================Sample===================================>>>>>>>>>>>>>>\\n\\n \" + generated_seq)\n","\n","\n","if __name__ == \"__main__\":\n","\tsequence_gen()\n"],"metadata":{"id":"K_fga9t1iwyr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"nG8grzRCjCi1"}},{"cell_type":"code","source":[""],"metadata":{"id":"m9KDO_yqjDVf"},"execution_count":null,"outputs":[]}]}